{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from random import choice\n",
    "from scipy.stats import bernoulli\n",
    "from numpy.linalg import eig\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load binary classification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features, data_labels = load_svmlight_file(\"a4a.txt\")\n",
    "\"\"\"\n",
    "converting sparse matrix (which is in tuple form) to a dense matrix\n",
    "\"\"\"\n",
    "A_dense = data_features.todense()\n",
    "\n",
    "A_dense = np.array(A_dense)\n",
    "\n",
    "print (type(A_dense))\n",
    "print (type(data_labels))\n",
    "\n",
    "num_samples = A_dense.shape[0]\n",
    "num_features = A_dense.shape[1]\n",
    "print (data_labels)\n",
    "print('num samples: %d num features : %d ' %(num_samples, num_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle and normalize the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "A = A_dense[indices]\n",
    "b = data_labels[indices]\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    A[i] = (1/LA.norm(A[i]))*A[i]\n",
    "\n",
    "print ('Minimum feature value =',np.min(A))\n",
    "print ('Maximum feature value =',np.max(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribute data points among nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_blocks(A,b,m):\n",
    "    \"A = feature matrix, b = target values, m = number of nodes\"\n",
    "    features = [[] for i in range(m)]\n",
    "    values = [[] for i in range(m)]\n",
    "    N = A.shape[0] ## N = number of samples\n",
    "    indices = np.arange(N)\n",
    "    size = int(N/m)  ## size of each block\n",
    "    for i in range(m):\n",
    "        start_idx = i*size\n",
    "        end_idx = min(N,(i+1)*size)\n",
    "        features[i] = A[indices[start_idx:end_idx]]\n",
    "        values[i] = b[indices[start_idx:end_idx]]\n",
    "    \n",
    "    samples_after_eq_dist = m*int(N/m)  ## total samples after equally distributed samples to every node\n",
    "    remaining_samples = N - m*int(N/m) ## This quantity will always be less than m\n",
    "    if ( remaining_samples >= 1):\n",
    "        for j in range(remaining_samples):\n",
    "            features[j] = np.vstack((features[j],A[samples_after_eq_dist + j]))\n",
    "            values[j] = np.hstack((values[j],b[samples_after_eq_dist + j]))\n",
    "\n",
    "    return features,values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create n mini-batches of local data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(local_features,local_values,num_batches):\n",
    "    \"num_batches = number of mini-batches\"\n",
    "    batch_features = [[] for i in range(num_batches)]\n",
    "    batch_values = [[] for i in range(num_batches)]\n",
    "    samples = len(local_features)\n",
    "    indices = np.arange(samples)\n",
    "    batch_size = int(samples/num_batches)\n",
    "    for batch in range(num_batches):\n",
    "        start_idx = batch*batch_size\n",
    "        end_idx = min(samples,(batch+1)*batch_size)\n",
    "        batch_features[batch] = local_features[start_idx:end_idx]\n",
    "        batch_values[batch] = local_values[start_idx:end_idx]\n",
    "    samples_after_eq_dist = num_batches*int(samples/num_batches)\n",
    "    remaining_samples = samples - samples_after_eq_dist ## This quantity will always be less than m\n",
    "    if ( remaining_samples >= 1):\n",
    "        for j in range(remaining_samples):\n",
    "            batch_features[j] = np.vstack((batch_features[j],local_features[samples_after_eq_dist+j])) ## remove indices\n",
    "            batch_values[j] = np.hstack((batch_values[j],local_values[samples_after_eq_dist+j]))\n",
    "     \n",
    "    return batch_features,batch_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute minibatch of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_mini_batches(features,values,nodes,num_batches):\n",
    "    \"features , values = local data points of all nodes \"\n",
    "    mini_batch_features = [[] for i in range(nodes)]\n",
    "    mini_batch_values = [[] for i in range(nodes)]\n",
    "    shuffled_features = [[] for i in range(nodes)]\n",
    "    shuffled_values = [[] for i in range(nodes)]\n",
    "    for i in range(nodes):\n",
    "        \"shuffling local samples befor creating mini-batches\"\n",
    "        local_num_samples = features[i].shape[0]\n",
    "        indices = np.arange(local_num_samples)\n",
    "        np.random.seed(i+10) ## For reproducimg same minibatches for comparative algorithms\n",
    "        np.random.shuffle(indices)\n",
    "        shuffled_features[i] = features[i][indices]\n",
    "        shuffled_values[i] = values[i][indices]\n",
    "        mini_batch_features[i],mini_batch_values[i] = create_mini_batches(shuffled_features[i]\n",
    "                                                                          ,shuffled_values[i],\n",
    "                                                                        num_batches)\n",
    "        \n",
    "    return mini_batch_features, mini_batch_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Lipschitz parameters of fij. For more details, please refer to Appendix XX-C in our technical report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding Lipschitz parameters of fij\n",
    "def local_lipschitz_constants(batch_features,batch_values, regcoef_x\n",
    "                              ,num_batches,scaling_factor,positive_samples,nodes):\n",
    "    \"This function returns the Lipschitz parameter of f_ij(x,y)\"\n",
    "    \n",
    "    number_samples = len(batch_features)  ## batch size\n",
    "    constant_n = 2*(1-positive_samples)  ## 2*(1-q)\n",
    "    constant_p = 2*positive_samples  ## 2*q\n",
    "    sum_tildeLxx = 0\n",
    "    sum_tildeLyy = 0\n",
    "    sum_tildeLxy = 0\n",
    "    for i in range(number_samples):\n",
    "        if (batch_values[i] == 1):\n",
    "            norm_i = LA.norm(batch_features[i])\n",
    "            sum_tildeLxx += constant_n*(norm_i**2) + constant_n\n",
    "            sum_tildeLyy += constant_n*positive_samples\n",
    "            sum_tildeLxy += constant_n*norm_i\n",
    "        else:\n",
    "            norm_i = LA.norm(batch_features[i])\n",
    "            sum_tildeLxx += constant_p*(norm_i**2) + constant_p\n",
    "            sum_tildeLyy += constant_n*positive_samples\n",
    "            sum_tildeLxy += constant_p*norm_i\n",
    "            \n",
    "    local_Lxx = (num_batches/scaling_factor)*sum_tildeLxx + regcoef_x/nodes\n",
    "    local_Lyy = (num_batches/scaling_factor)*sum_tildeLyy\n",
    "    local_Lxy = (num_batches/scaling_factor)*sum_tildeLxy\n",
    "    local_Lyx = local_Lxy\n",
    "    \n",
    "    return local_Lxx, local_Lyy, local_Lxy, local_Lyx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_lipschitz(mini_batch_features,mini_batch_values,nodes\n",
    "                    ,num_batches,regcoef_x,scaling_factor,positive_samples):\n",
    "    Lxx_batch = np.zeros(num_batches)\n",
    "    Lyy_batch = np.zeros(num_batches)\n",
    "    Lxy_batch = np.zeros(num_batches)\n",
    "    Lyx_batch = np.zeros(num_batches)\n",
    "    Lxx_nodes = np.zeros(nodes)\n",
    "    Lyy_nodes = np.zeros(nodes)\n",
    "    Lxy_nodes = np.zeros(nodes)\n",
    "    Lyx_nodes = np.zeros(nodes)\n",
    "    \n",
    "    for i in range(nodes):\n",
    "        for j in range(num_batches):\n",
    "            Lxx_batch[j],Lyy_batch[j],Lxy_batch[j],Lyx_batch[j] = local_lipschitz_constants(\n",
    "                                            mini_batch_features[i][j],mini_batch_values[i][j],\n",
    "                                            regcoef_x,num_batches,scaling_factor,positive_samples,\n",
    "                                                nodes)   \n",
    "        Lxx_nodes[i] = np.max(Lxx_batch)\n",
    "        Lyy_nodes[i] = np.max(Lyy_batch)\n",
    "        Lxy_nodes[i] = np.max(Lxy_batch)\n",
    "        Lyx_nodes[i] = np.max(Lyx_batch)\n",
    "        \n",
    "    return np.max(Lxx_nodes), np.max(Lyy_nodes),np.max(Lxy_nodes), np.max(Lyx_nodes)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AUC function value computed at (x,u,v,y)\n",
    "\"\"\"\n",
    "\n",
    "def auc_func(A,b,primal_x,dual_y,regcoef_x,scaling_factor,positive_samples):\n",
    "    size = A.shape[1] + 2\n",
    "    x = primal_x[:A.shape[1]]\n",
    "    u = primal_x[size - 2]\n",
    "    v = primal_x[-1]\n",
    "    y = dual_y[0]\n",
    "    func = 0\n",
    "    N = A.shape[0] ## number of samples\n",
    "    for i in range(N):\n",
    "        inner_prod = np.dot(A[i],x)\n",
    "        if (b[i] == 1):\n",
    "            func += (1-positive_samples)*(inner_prod - u)**2 - 2*(1+y)*(1-positive_samples)*inner_prod\n",
    "        else:\n",
    "            func += positive_samples*(inner_prod - v)**2 + 2*(1+y)*positive_samples*inner_prod\n",
    "         \n",
    "    function = func/scaling_factor - positive_samples*(1-positive_samples)*(y**2) + 0.5*regcoef_x*(LA.norm(x))**2\n",
    "\n",
    "    return function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function returns the gradient of f_i with respect to (x,u,v) and y.\n",
    "\n",
    "def full_batch_gradient(FB_features,FB_values,regcoef_x,scaling_factor,positive_samples,primal_x,dual_y):\n",
    "    \n",
    "    \"FB_features,FB_values = features, lables available at node i\"\n",
    "    size = FB_features.shape[1] + 2 ## dimension of primal variable (x,u,v)\n",
    "    \n",
    "    'separating x, u and v'\n",
    "    x = primal_x[:FB_features.shape[1]]\n",
    "    u = primal_x[size - 2]\n",
    "    v = primal_x[-1]\n",
    "    y = dual_y[0] ## dual_y is of the form [scalar]\n",
    "    grad_x = np.zeros(FB_features.shape[1])\n",
    "    grad_u = 0  ## because u is scalar\n",
    "    grad_v = 0  ## because v is scalar\n",
    "    grad_y = 0  ## because y is scalar\n",
    "    local_samples = FB_features.shape[0] ## Number of local samples N_i\n",
    "    for i in range(local_samples):\n",
    "        inner_prod = np.dot(FB_features[i],x)\n",
    "        if (FB_values[i] == 1):\n",
    "            scalar_1 = 2*(1-positive_samples)*(inner_prod - u)\n",
    "            scalar_2 = (-2)*(1+y)*(1-positive_samples)\n",
    "            grad_x += (scalar_1 + scalar_2)*FB_features[i]\n",
    "            grad_u += (-2)*(1-positive_samples)*(inner_prod - u)\n",
    "#             print ('grad_u:',grad_u)\n",
    "            grad_y += (-2)*positive_samples*(1-positive_samples)*y - 2*(1-positive_samples)*inner_prod\n",
    "            \n",
    "        else:\n",
    "            scalar_1 = 2*positive_samples*(inner_prod - v)\n",
    "            scalar_2 = 2*(1+y)*positive_samples\n",
    "            grad_x += (scalar_1 + scalar_2)*FB_features[i]\n",
    "            grad_v += (-2)*positive_samples*(inner_prod - v)\n",
    "#             print ('grad_v:',grad_v)\n",
    "            grad_y += (-2)*positive_samples*(1-positive_samples)*y + 2*positive_samples*inner_prod\n",
    "            \n",
    "    gradient_x = (1/scaling_factor)*grad_x + (regcoef_x/nodes)*x\n",
    "    gradient_u = (1/scaling_factor)*grad_u  \n",
    "    gradient_v = (1/scaling_factor)*grad_v    \n",
    "    gradient_y = (1/scaling_factor)*grad_y  \n",
    "    grad_x_u = np.append(gradient_x,gradient_u)\n",
    "    \n",
    "    gradient_primal = np.append(grad_x_u,gradient_v)\n",
    "    \n",
    "    gradient_dual = np.array([gradient_y]) ## making gradient w.r.t y into numpy array format\n",
    "\n",
    "    return gradient_primal , gradient_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function returns the gradient of f_ij(x,y) with respect to x,u,v and y.\n",
    "## primal variables = x,u,v, dual variable = y\n",
    "\n",
    "def mini_batch_gradient(batch_features,batch_values,regcoef_x,num_batches,scaling_factor,\n",
    "                        positive_samples,primal_x,dual_y):\n",
    "    size = batch_features.shape[1] + 2\n",
    "    x = primal_x[:batch_features.shape[1]]\n",
    "    u = primal_x[size - 2]\n",
    "    v = primal_x[-1]\n",
    "    y = dual_y[0]\n",
    "    grad_x = np.zeros(batch_features.shape[1])\n",
    "    grad_u = 0\n",
    "    grad_v = 0\n",
    "    grad_y = 0\n",
    "    local_samples = batch_features.shape[0] ## batch size: N_ij\n",
    "    for i in range(local_samples):\n",
    "        inner_prod = np.dot(batch_features[i],x)\n",
    "        if (batch_values[i] == 1):\n",
    "            scalar_1 = 2*(1-positive_samples)*(inner_prod - u)\n",
    "            scalar_2 = (-2)*(1+y)*(1-positive_samples)\n",
    "            grad_x += (scalar_1 + scalar_2)*batch_features[i]\n",
    "            grad_u += (-2)*(1-positive_samples)*(inner_prod - u)\n",
    "            grad_y += (-2)*positive_samples*(1-positive_samples)*y - 2*(1-positive_samples)*inner_prod\n",
    "            \n",
    "        else:\n",
    "            scalar_1 = 2*positive_samples*(inner_prod - v)\n",
    "            scalar_2 = 2*(1+y)*positive_samples\n",
    "            grad_x += (scalar_1 + scalar_2)*batch_features[i]\n",
    "            grad_v += (-2)*positive_samples*(inner_prod - v)\n",
    "            grad_y += (-2)*positive_samples*(1-positive_samples)*y + 2*positive_samples*inner_prod\n",
    "            \n",
    "            \n",
    "    gradient_x = (num_batches/scaling_factor)*grad_x + (regcoef_x/nodes)*x\n",
    "    gradient_u = (num_batches/scaling_factor)*grad_u  \n",
    "    gradient_v = (num_batches/scaling_factor)*grad_v  \n",
    "  \n",
    "    gradient_y = (num_batches/scaling_factor)*grad_y   \n",
    " \n",
    "    gradient_dual = np.array([gradient_y])\n",
    "    grad_x_u = np.append(gradient_x,gradient_u)\n",
    "    gradient_primal = np.append(grad_x_u,gradient_v)\n",
    "    \n",
    "    return gradient_primal , gradient_dual\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This function returns the stochastic gradient estimator and latest reference point for a particular node\"\n",
    "\n",
    "def SGD_oracle(batches_features,batches_values,sample_prob ,nodes,num_batches,primal_x,dual_y,\n",
    "               regcoef_x,scaling_factor,positive_samples):\n",
    "    \n",
    "    \"sample_prob = probability of sampling a mini-batch\"\n",
    "    \"batches_features , batches_values = batches of a particular node\"\n",
    "    indices = np.arange(num_batches)\n",
    "    sampled_batch_list = np.random.choice(indices,1,p = sample_prob) ## sampling a minibatch using sample_prob distribution\n",
    "    sampled_batch = sampled_batch_list[0]\n",
    "\n",
    "    mini_batch_f = batches_features[sampled_batch] \n",
    "    mini_batch_val = batches_values[sampled_batch] \n",
    "\n",
    "    stoch_grad_primal, stoch_grad_y = mini_batch_gradient(mini_batch_f,mini_batch_val,regcoef_x,\n",
    "                                                    num_batches,scaling_factor,positive_samples,\n",
    "                                                    primal_x,dual_y)\n",
    "\n",
    "    return stoch_grad_primal, stoch_grad_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This function returns the stochastic gradient estimator and latest reference point for a particular node\"\n",
    "\n",
    "def svrg_oracle(features,values,batches_features,batches_values,sample_prob\n",
    "               ,omega,num_batches,FB_grad_primal, FB_grad_y,ref_point_primal,ref_point_y,primal_x,\n",
    "                dual_y,scaling_factor,positive_samples):\n",
    "    \"sample_prob = probability of sampling a mini-batch\"\n",
    "    \"ref_prob = probability with which refrence point is updated\"\n",
    "    \"features, values = full batch features and values at a given node\"\n",
    "    \"batches_features , batches_values = batches of a particular node\"\n",
    "    indices = np.arange(num_batches)\n",
    "    sampled_batch_list = np.random.choice(indices,1,p = sample_prob) ## sampling a minibatch using sample_prob distribution\n",
    "    sampled_batch = sampled_batch_list[0]\n",
    "    mini_batch_f = batches_features[sampled_batch] \n",
    "    mini_batch_val = batches_values[sampled_batch] \n",
    "    \n",
    "    grad_primal, grad_y = mini_batch_gradient(mini_batch_f,mini_batch_val,regcoef_x,num_batches,\n",
    "                                        scaling_factor,positive_samples,primal_x,dual_y)\n",
    "    \n",
    "    ## computes gradient at (ref_point_primal,ref_point_y)\n",
    "    grad_ref_primal, grad_ref_y = mini_batch_gradient(mini_batch_f,mini_batch_val,regcoef_x,num_batches,\n",
    "                                    scaling_factor,positive_samples,ref_point_primal,ref_point_y)\n",
    "    \n",
    "    # grad_x f_il(x,y) - grad_x f_il(ref_x,ref_y)\n",
    "    grad_primal_diff = np.subtract(grad_primal, grad_ref_primal)\n",
    "    ## computing 1/np_il * (grad_x f_il(x,y) - grad_x f_il(ref_x,ref_y))\n",
    "    scale_grad_primal_diff = 1/(num_batches*sample_prob[sampled_batch])*grad_primal_diff\n",
    "    stoch_grad_primal = np.add(scale_grad_primal_diff, FB_grad_primal)\n",
    "    \n",
    "    ## computing stochastic gradient with respect to y\n",
    "    \n",
    "    # grad_y f_il(x,y) - grad_y f_il(ref_x,ref_y)\n",
    "    grad_y_diff = np.subtract(grad_y, grad_ref_y)\n",
    "    ## computing 1/np_il * (grad_y f_il(x,y) - grad_y f_il(ref_x,ref_y))\n",
    "    scale_grad_y_diff = 1/(num_batches*sample_prob[sampled_batch])*grad_y_diff\n",
    "    stoch_grad_y = np.add(scale_grad_y_diff, FB_grad_y)\n",
    "    \n",
    "#     \"updating reference point\"\n",
    "    if (omega == 1):\n",
    "        ref_point_primal = np.copy(primal_x)\n",
    "        ref_point_y = np.copy(dual_y)\n",
    "        ## computes full batch gradient at new (ref_point_x,ref_point_y)\n",
    "        \n",
    "        FB_grad_primal, FB_grad_y = full_batch_gradient(features,values,regcoef_x,scaling_factor,positive_samples,\n",
    "                                                   ref_point_primal,ref_point_y)\n",
    "        \n",
    "    return stoch_grad_primal, stoch_grad_y,FB_grad_primal, FB_grad_y, ref_point_primal,ref_point_y\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns the weighted sum of x_1, x_2, ...,x_m where x = [x_1,...,x_m]\n",
    "\"\"\"\n",
    "\n",
    "def oneConsensus(W,nodes,x):\n",
    "    v = np.zeros((nodes,len(x[0])))\n",
    "    for i in range(nodes):\n",
    "        u = [W[i][j]*np.array(x[j]) for j in range(nodes)]\n",
    "        u = np.array(u)\n",
    "        v[i] = u.sum(axis = 0)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## faster AccGossip. Fast for higher dimensions\n",
    "## This module returns average of vectors\n",
    "\n",
    "def acce_consensus_ite(W,m,eta,tau,x):\n",
    "    \n",
    "    v = np.zeros((nodes,len(x[0])))\n",
    "\n",
    "    x_new1 = np.copy(x)\n",
    "    x_old1 = np.copy(x)\n",
    "\n",
    "    for t in range(int(tau)):\n",
    "        x_old2 = np.copy(x_old1)   ## z_k,t-1\n",
    "        x_old1 = np.copy(x_new1)   ## z_k,t\n",
    "        for i in range(m):\n",
    "            v2 = [W[i][j]*np.array(x_old1[j]) for j in range(m)]\n",
    "            v2 = np.array(v2)\n",
    "            v[i] = v2.sum(axis = 0)\n",
    "        first_term = (1+eta)*np.array(v)\n",
    "        sec_term = eta*np.array(x_old2)\n",
    "        x_new1 = np.subtract(first_term, sec_term)  ## z_{k,t+1}\n",
    "    return x_new1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## faster AccGossip. Fast for higher dimensions\n",
    "## x = [x1,x2,....,xm], m-dimensional vector\n",
    "## This module returns average of scalars\n",
    "\n",
    "def acce_consensus(W,m,eta,tau,x):\n",
    "    \n",
    "    v = np.zeros(nodes)\n",
    "\n",
    "    x_new1 = np.copy(x)\n",
    "    x_old1 = np.copy(x)\n",
    "\n",
    "    for t in range(int(tau)):\n",
    "        x_old2 = np.copy(x_old1)   ## z_k,t-1\n",
    "        x_old1 = np.copy(x_new1)   ## z_k,t\n",
    "        for i in range(m):\n",
    "#             v2 = np.dot(W[i],x_old1)\n",
    "#           v2 =  [W[i][j]*x_old1[j] for j in range(m)]\n",
    "#             v2 = np.array(v2)\n",
    "#             v[i] = np.sum(v2)\n",
    "            v[i] = np.dot(W[i],x_old1)\n",
    "#         first_term = (1+eta)*np.array(v)\n",
    "#         sec_term = eta*np.array(x_old2)\n",
    "        first_term = (1+eta)*v\n",
    "        sec_term = eta*x_old2\n",
    "        \n",
    "        x_new1 = np.subtract(first_term, sec_term)  ## z_{k,t+1}\n",
    "    return x_new1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Projection of point v onto l2 ball = (radius*v)/max(radius,||v||)\n",
    "\"\"\"\n",
    "\n",
    "def projection_L2ball(v,radius):\n",
    "    norm = LA.norm(v)\n",
    "    if (norm <= radius):\n",
    "        return v\n",
    "    else:\n",
    "        scaling = radius/norm\n",
    "        projection = scaling*v\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This function returns the sum of difference between xt ,xstar and yt, ystar\"\n",
    "def distance_from_saddle(x,y,xstar,ystar,nodes):\n",
    "    dist_xi_xstar = 0\n",
    "    dist_yi_ystar = 0\n",
    "    for i in range(nodes):\n",
    "        diff_xi = np.subtract(x[i],xstar)\n",
    "        dist_xi_xstar += (LA.norm(diff_xi))**2\n",
    "        diff_yi = np.subtract(y[i],ystar)\n",
    "        dist_yi_ystar += (LA.norm(diff_yi))**2\n",
    "    total_distance = dist_xi_xstar +  dist_yi_ystar   \n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_distances(x,y,x0,y0):\n",
    "#     distances = np.zeros(nodes)\n",
    "    diff_x = np.subtract(x,x0)\n",
    "    norm_x = LA.norm(diff_x,axis = 1) ## computes norm of each x[i] - x0[i]\n",
    "    square_norm_x = np.square(norm_x) ## takes elementwise square of ||x[i] - x0[i]||\n",
    "    \n",
    "    diff_y = np.subtract(y,y0)\n",
    "    norm_y = LA.norm(diff_y, axis = 1) ## computes norm of each y[i] - y0[i]\n",
    "    square_norm_y = np.square(norm_y) ## takes elementwise square of ||y[i] - y0[i]||\n",
    "    \n",
    "    distances = np.add(square_norm_x, square_norm_y) ## ith entry: ||x[i] - x0[i]||^2 + ||y[i] - y0[i]||^2\n",
    "    \n",
    "    return distances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compression operator\n",
    "\n",
    "def qsgd_quantize(x, num_bits): \n",
    "    bits = 2**(num_bits - 1)\n",
    "    norm = LA.norm(x, np.inf)\n",
    "    if (norm <= 10**(-15)): ## if x is zero vector\n",
    "        return x\n",
    "    else:\n",
    "        level_float = bits * np.abs(x) / norm\n",
    "        previous_level = np.floor(level_float)\n",
    "        is_next_level = np.random.rand(*x.shape) < (level_float - previous_level)\n",
    "        new_level = previous_level + is_next_level\n",
    "        return np.sign(x) * norm * new_level / bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_qsgd(num_bits,dimension):\n",
    "    \n",
    "    bit_rep = 2**(num_bits - 1)\n",
    "    delta = dimension/(4*bit_rep**2)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function compresses x_1,..x_m and y_1,...,y_m.\n",
    "# Then compressed vectors are communicated among nodes to get WQ(x).\n",
    "\n",
    "def COMM(num_bits,nu_x,H_x,Hw_x,alpha,W,nodes):\n",
    "    ## initializing Q_x, Q_y\n",
    "    Q_x = np.copy(nu_x) \n",
    "    for i in range(nodes):\n",
    "        diff_x = np.subtract(nu_x[i],H_x[i])\n",
    "        Q_x[i]= qsgd_quantize(diff_x, num_bits)\n",
    " \n",
    "    nu_hat_x = np.add(H_x,Q_x) \n",
    "    \n",
    "    new_H_x = np.add((1-alpha)*H_x, alpha*nu_hat_x)\n",
    "    \n",
    "    WQ_x = oneConsensus(W,nodes,Q_x)\n",
    "   \n",
    "    nuW_hat_x = np.add(Hw_x,WQ_x)\n",
    "  \n",
    "    new_Hw_x = np.add((1-alpha)*Hw_x,alpha*nuW_hat_x)\n",
    "    \n",
    "    return nu_hat_x, nuW_hat_x, new_H_x, new_Hw_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_SGD(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,Kmax):\n",
    "    \n",
    "    \"features, values = local samples of all nodes\"\n",
    "    \"mini_batch_features, mini_batch_values = mini-batches of all nodes \"\n",
    "    dist_from_saddle = np.zeros(Kmax+1)\n",
    "    func_avg = np.zeros(Kmax+1)\n",
    "    auc_val = np.zeros(Kmax+1)\n",
    "    consensus_error_x = np.zeros(Kmax)\n",
    "    consensus_error_y = np.zeros(Kmax)\n",
    "    \n",
    "    compression_error_x = np.zeros(Kmax)\n",
    "    compression_error_y = np.zeros(Kmax)\n",
    "\n",
    "    x_t = np.copy(x0) ## current iterate x_t\n",
    "    y_t = np.copy(y0) ## current iterate y_t\n",
    "    xt_hat = np.copy(x0)\n",
    "    yt_hat = np.copy(y0)\n",
    "    nu_xt = np.copy(x0)\n",
    "    nu_yt = np.copy(y0)\n",
    "    Dx_t = np.copy(D_x)\n",
    "    Dy_t = np.copy(D_y)\n",
    "    Hx_t = np.copy(H_x)\n",
    "    Hy_t = np.copy(H_y)\n",
    "    Hw_xt = np.copy(Hw_x)\n",
    "    Hw_yt = np.copy(Hw_y)\n",
    "    \n",
    "    access_w = x_t[0][:A.shape[1]]\n",
    "    auc_val[0] = auc_curve(A_dense,data_labels,access_w)\n",
    "    AUC_val.write(str(auc_val[0])+'\\n')\n",
    "    AUC_val.flush()\n",
    "    \n",
    "    ## saving distance from saddle point solution\n",
    "    dist_from_saddle[0] = distance_from_saddle(x_t,y_t,xstar,ystar,nodes)\n",
    "    total_distance_from_saddle.write(str(dist_from_saddle[0])+'\\n')\n",
    "    total_distance_from_saddle.flush()\n",
    "    \n",
    "    func_avg[0] = auc_func(A,b,x_t[0],y_t[0],regcoef_x,scaling_factor,positive_samples)\n",
    "    \n",
    "    function_values.write(str(func_avg[0])+'\\n')\n",
    "    function_values.flush()\n",
    "     \n",
    "    \"computing constants alpha, gamma\"\n",
    "    \n",
    "    \n",
    "    bx, by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize)\n",
    "\n",
    "    alpha_x, alpha_y, gamma_x, gamma_y, Mx,My = sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,Lxx,\n",
    "                                                    Lyy,Lxy,Lyx,delta,nodes,lambda_max)    \n",
    "#     print ('alpha_x, alpha_y, gamma_x, gamma_y',alpha_x, alpha_y, gamma_x, gamma_y)\n",
    "    for t in range(Kmax):\n",
    "        ## computing gradient steps\n",
    "        for i in range(nodes):\n",
    "            ## compute stochastic gradient estimator using svrg oracle\n",
    "            \n",
    "            stoch_grad_x, stoch_grad_y = SGD_oracle(mini_batch_features[i],mini_batch_values[i],\n",
    "                                            sample_prob , nodes,num_batches,x_t[i],y_t[i],regcoef_x,\n",
    "                                            scaling_factor,positive_samples)\n",
    "                        \n",
    "            direction_x = np.add(stoch_grad_x, Dx_t[i])\n",
    "            nu_xt[i] = np.subtract(x_t[i],sgd_stepsize*direction_x)\n",
    "\n",
    "            ## finding nu_yt \n",
    "            direction_y = np.subtract(stoch_grad_y, Dy_t[i])\n",
    "            nu_yt[i] = np.add(y_t[i],sgd_stepsize*direction_y)\n",
    "            \n",
    "        \"compressing nu_xt and nu_yt and communicate the resulting compressed vectors\"   \n",
    "        nu_hat_xt, nuW_hat_xt, Hx_t,Hw_xt = COMM(num_bits,nu_xt,Hx_t,Hw_xt,alpha_x,W,nodes)\n",
    "        nu_hat_yt, nuW_hat_yt, Hy_t,Hw_yt = COMM(num_bits,nu_yt,Hy_t,Hw_yt,alpha_y,W,nodes)\n",
    "        \n",
    "        \n",
    "        diff_hat_nux_nuxW = np.subtract(nu_hat_xt,nuW_hat_xt)\n",
    "        diff_hat_nuy_nuyW = np.subtract(nu_hat_yt,nuW_hat_yt)\n",
    "        \n",
    "        ## updating Dxt and Dyt\n",
    "        \n",
    "        scale_diff_nux_nuxW = (gamma_x/(2*sgd_stepsize))*diff_hat_nux_nuxW\n",
    "        scale_diff_nuy_nuyW = (gamma_y/(2*sgd_stepsize))*diff_hat_nuy_nuyW\n",
    "        \n",
    "        Dx_t = np.add(Dx_t,scale_diff_nux_nuxW)\n",
    "        Dy_t = np.add(Dy_t,scale_diff_nuy_nuyW)\n",
    "        \n",
    "        ## upating xt_hat and yt_hat\n",
    "        \n",
    "        gamma_diff_nux_nuxW = (gamma_x/2)*diff_hat_nux_nuxW\n",
    "        gamma_diff_nuy_nuyW = (gamma_y/2)*diff_hat_nuy_nuyW\n",
    "        \n",
    "        xt_hat = np.subtract(nu_xt,gamma_diff_nux_nuxW)\n",
    "        yt_hat = np.subtract(nu_yt,gamma_diff_nuy_nuyW)\n",
    "        for i in range(nodes):\n",
    "            x_t[i] = projection_L2ball(xt_hat[i],radius_x)\n",
    "            y_t[i] = projection_L2ball(yt_hat[i],radius_y) \n",
    "        \n",
    "        \"To compute zT0 - zT0-1, copy xT0-1, yT0-1\"\n",
    "        if (t == Kmax-2):\n",
    "            prev_xt = np.copy(x_t)\n",
    "            prev_yt = np.copy(y_t)\n",
    "        \n",
    "        \"Saving required quantities\"\n",
    "        \n",
    "        x_ti_avg = x_t.mean(axis = 0) ## cumputes mean of local iterates x_t[i] over i\n",
    "\n",
    "        y_ti_avg = y_t.mean(axis = 0) ## cumputes mean of local iterates y_t[i] over i\n",
    "        \n",
    "        \"AUC value\"\n",
    "        access_w = x_ti_avg[:A.shape[1]]\n",
    "        auc_val[t+1] = auc_curve(A_dense,data_labels,access_w)\n",
    "        AUC_val.write(str(auc_val[t+1])+'\\n')\n",
    "        AUC_val.flush() \n",
    "        \n",
    "        \n",
    "        func_avg[t+1] = auc_func(A,b,x_ti_avg,y_ti_avg,regcoef_x,scaling_factor,positive_samples)\n",
    "\n",
    "        function_values.write(str(func_avg[t+1])+'\\n')\n",
    "        function_values.flush()  \n",
    "        \n",
    "        ## saving distance from saddle point solution\n",
    "        dist_from_saddle[t+1] = distance_from_saddle(x_t,y_t,xstar,ystar,nodes)\n",
    "        total_distance_from_saddle.write(str(dist_from_saddle[t+1])+'\\n')\n",
    "        total_distance_from_saddle.flush()\n",
    "        \n",
    "        ## saving consensus error\n",
    "        cons_error_x = 0\n",
    "        cons_error_y = 0\n",
    "        for i in range(nodes):\n",
    "            consensus_x = np.subtract(x_t[i],x_ti_avg)\n",
    "            cons_error_x += (LA.norm(consensus_x))**2\n",
    "            consensus_y = np.subtract(y_t[i],y_ti_avg)\n",
    "            cons_error_y += (LA.norm(consensus_y))**2\n",
    "            \n",
    "        consensus_error_x[t] = cons_error_x\n",
    "        consensus_error_y[t] = cons_error_y\n",
    "        sum_consensus_error_x.write(str(consensus_error_x[t])+'\\n')\n",
    "        sum_consensus_error_x.flush()\n",
    "        sum_consensus_error_y.write(str(consensus_error_y[t])+'\\n')\n",
    "        sum_consensus_error_y.flush()\n",
    "        \n",
    "         ## saving compression error nuxhat - nux  and nuyhat - nuy\n",
    "        \n",
    "        comp_error_x = np.subtract(nu_hat_xt,nu_xt)\n",
    "        compression_error_x[t] = sum([(LA.norm(comp_error_x[i]))**2 for i in range(nodes)])\n",
    "        compression_error_nux.write(str(compression_error_x[t])+'\\n')\n",
    "        compression_error_nux.flush()\n",
    "        \n",
    "        comp_error_y = np.subtract(nu_hat_yt,nu_yt)\n",
    "        compression_error_y[t] = sum([(LA.norm(comp_error_y[i]))**2 for i in range(nodes)])\n",
    "        compression_error_nuy.write(str(compression_error_y[t])+'\\n')\n",
    "        compression_error_nuy.flush()\n",
    "        \n",
    "        \"Saving completed!\"\n",
    "    \n",
    "    dist_local_zt_z0 = local_distances(x_t,y_t,prev_xt,prev_yt)\n",
    "    avg_sgd_dist = acce_consensus(W,nodes,eta,tau,dist_local_zt_z0)\n",
    "    full_sgd_distances = {'full_sgd_loc_dist':dist_local_zt_z0,'full_sgd_avg_dist':avg_sgd_dist}\n",
    "        \n",
    "    return x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt,Hw_yt,full_sgd_distances\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_SVRG(A,b,x0,y0,Dx,Dy,Hx,Hy,Hwx,Hwy,svrg_stepsize,T):\n",
    "    \n",
    "    \n",
    "    \"features, values = local samples of all nodes\"\n",
    "    \"mini_batch_features, mini_batch_values = mini-batches of all nodes \"\n",
    "    dist_from_saddle = np.zeros(T+1)\n",
    "    func_avg = np.zeros(T+1)\n",
    "    auc_val = np.zeros(T+1)\n",
    "\n",
    "    consensus_error_x = np.zeros(T)\n",
    "    consensus_error_y = np.zeros(T)\n",
    "    \n",
    "    compression_error_x = np.zeros(T)\n",
    "    compression_error_y = np.zeros(T) \n",
    "    \n",
    "    x_t = np.copy(x0) ## current iterate x_t\n",
    "    y_t = np.copy(y0) ## current iterate y_t\n",
    "    xt_hat = np.copy(x0)\n",
    "    yt_hat = np.copy(y0)\n",
    "    nu_xt = np.copy(x0)\n",
    "    nu_yt = np.copy(y0)\n",
    "    Dx_t = np.copy(Dx)\n",
    "    Dy_t = np.copy(Dy)\n",
    "    Hx_t = np.copy(Hx)\n",
    "    Hy_t = np.copy(Hy)\n",
    "    Hw_xt = np.copy(Hwx)\n",
    "    Hw_yt = np.copy(Hwy)\n",
    "\n",
    "\n",
    "    \"reference points are initialized to the last primal and dual iterates of IPDHG + SGD oracle\"\n",
    "    ref_point_x = np.copy(x_t)\n",
    "    ref_point_y = np.copy(y_t)\n",
    "    \n",
    "    ## Initial full batch gradient\n",
    "    FB_grad_x = [[] for i in range(nodes)]\n",
    "    FB_grad_y = [[] for i in range(nodes)]\n",
    "    for i in range(nodes):\n",
    "        \n",
    "        FB_grad_x[i],FB_grad_y[i] = full_batch_gradient(features[i],values[i],regcoef_x,\n",
    "                            scaling_factor,positive_samples,ref_point_x[i],ref_point_y[i])\n",
    "        \n",
    "    omega = np.zeros(T) ## keeps track of full batch gradient computations\n",
    "    \n",
    "    \"parameters setup for svrg\"\n",
    "    \n",
    "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
    "\n",
    "\n",
    "    alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(\n",
    "                                                    mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W)\n",
    "\n",
    "    Mx, My = MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta)\n",
    "    \n",
    "    for t in range(T):\n",
    "        ## generate a Bernoulli rv with prob ref_prob\n",
    "        omega[t] = bernoulli.rvs(ref_prob)\n",
    "        ## saving omega values\n",
    "        full_batch_grad_counts.write(str(omega[t])+'\\n')\n",
    "        full_batch_grad_counts.flush()\n",
    "        ## computing gradient steps\n",
    "        for i in range(nodes):\n",
    "            ## compute stochastic gradient estimator using svrg oracle\n",
    "    \n",
    "            stoch_grad_x, stoch_grad_y,FB_grad_x[i], FB_grad_y[i], ref_point_x[i],ref_point_y[i] = svrg_oracle(\n",
    "                                                                features[i],values[i],mini_batch_features[i],\n",
    "                                                 mini_batch_values[i],sample_prob, omega[t],num_batches,FB_grad_x[i], \n",
    "                                                FB_grad_y[i],ref_point_x[i], ref_point_y[i],\n",
    "                                                 x_t[i], y_t[i] ,scaling_factor,positive_samples)\n",
    "                        \n",
    "            direction_x = np.add(stoch_grad_x, Dx_t[i])\n",
    "            nu_xt[i] = np.subtract(x_t[i],svrg_stepsize*direction_x)\n",
    "\n",
    "            ## finding nu_yt \n",
    "            direction_y = np.subtract(stoch_grad_y, Dy_t[i])\n",
    "            nu_yt[i] = np.add(y_t[i],svrg_stepsize*direction_y)\n",
    "            \n",
    "        \"compressing nu_xt and nu_yt and communicate the resulting compressed vectors\"   \n",
    "        nu_hat_xt, nuW_hat_xt, Hx_t,Hw_xt = COMM(num_bits,nu_xt,Hx_t,Hw_xt,alpha_x,W,nodes)\n",
    "        nu_hat_yt, nuW_hat_yt, Hy_t,Hw_yt = COMM(num_bits,nu_yt,Hy_t,Hw_yt,alpha_y,W,nodes)\n",
    "        \n",
    "        \n",
    "        diff_hat_nux_nuxW = np.subtract(nu_hat_xt,nuW_hat_xt)\n",
    "        diff_hat_nuy_nuyW = np.subtract(nu_hat_yt,nuW_hat_yt)\n",
    "        \n",
    "        ## updating Dxt and Dyt\n",
    "        \n",
    "        scale_diff_nux_nuxW = (gamma_x/(2*svrg_stepsize))*diff_hat_nux_nuxW\n",
    "        scale_diff_nuy_nuyW = (gamma_y/(2*svrg_stepsize))*diff_hat_nuy_nuyW\n",
    "        \n",
    "        Dx_t = np.add(Dx_t,scale_diff_nux_nuxW)\n",
    "        Dy_t = np.add(Dy_t,scale_diff_nuy_nuyW)\n",
    "        \n",
    "        ## upating xt_hat and yt_hat\n",
    "        \n",
    "        gamma_diff_nux_nuxW = (gamma_x/2)*diff_hat_nux_nuxW\n",
    "        gamma_diff_nuy_nuyW = (gamma_y/2)*diff_hat_nuy_nuyW\n",
    "        \n",
    "        xt_hat = np.subtract(nu_xt,gamma_diff_nux_nuxW)\n",
    "        yt_hat = np.subtract(nu_yt,gamma_diff_nuy_nuyW)\n",
    "        for i in range(nodes):\n",
    "            x_t[i] = projection_L2ball(xt_hat[i],radius_x)\n",
    "            y_t[i] = projection_L2ball(yt_hat[i],radius_y) \n",
    "            \n",
    "        \"Saving required quantities\"\n",
    "        \n",
    "        x_ti_avg = x_t.mean(axis = 0) ## cumputes mean of local iterates x_t[i] over i\n",
    "\n",
    "        y_ti_avg = y_t.mean(axis = 0) ## cumputes mean of local iterates y_t[i] over i\n",
    "\n",
    "        \n",
    "        \"AUC value\"\n",
    "        access_w = x_ti_avg[:A.shape[1]]\n",
    "        auc_val[t+1] = auc_curve(A_dense,data_labels,access_w)\n",
    "        AUC_val.write(str(auc_val[t+1])+'\\n')\n",
    "        AUC_val.flush() \n",
    "        \n",
    "        \n",
    "        func_avg[t+1] = auc_func(A,b,x_ti_avg,y_ti_avg,regcoef_x,scaling_factor,positive_samples)  \n",
    "        function_values.write(str(func_avg[t+1])+'\\n')\n",
    "        function_values.flush() \n",
    "        ## saving distance from saddle point solution\n",
    "        dist_from_saddle[t+1] = distance_from_saddle(x_t,y_t,xstar,ystar,nodes)\n",
    "        total_distance_from_saddle.write(str(dist_from_saddle[t+1])+'\\n')\n",
    "        total_distance_from_saddle.flush()\n",
    "        \n",
    "        ## saving consensus error\n",
    "        cons_error_x = 0\n",
    "        cons_error_y = 0\n",
    "        for i in range(nodes):\n",
    "            consensus_x = np.subtract(x_t[i],x_ti_avg)\n",
    "            cons_error_x += (LA.norm(consensus_x))**2\n",
    "            consensus_y = np.subtract(y_t[i],y_ti_avg)\n",
    "            cons_error_y += (LA.norm(consensus_y))**2\n",
    "            \n",
    "        consensus_error_x[t] = cons_error_x\n",
    "        consensus_error_y[t] = cons_error_y\n",
    "        sum_consensus_error_x.write(str(consensus_error_x[t])+'\\n')\n",
    "        sum_consensus_error_x.flush()\n",
    "        sum_consensus_error_y.write(str(consensus_error_y[t])+'\\n')\n",
    "        sum_consensus_error_y.flush()\n",
    "        \n",
    "         ## saving compression error nuxhat - nux  and nuyhat - nuy\n",
    "        \n",
    "        comp_error_x = np.subtract(nu_hat_xt,nu_xt)\n",
    "        compression_error_x[t] = sum([(LA.norm(comp_error_x[i]))**2 for i in range(nodes)])\n",
    "        compression_error_nux.write(str(compression_error_x[t])+'\\n')\n",
    "        compression_error_nux.flush()\n",
    "        \n",
    "        comp_error_y = np.subtract(nu_hat_yt,nu_yt)\n",
    "        compression_error_y[t] = sum([(LA.norm(comp_error_y[i]))**2 for i in range(nodes)])\n",
    "        compression_error_nuy.write(str(compression_error_y[t])+'\\n')\n",
    "        compression_error_nuy.flush()\n",
    "        \n",
    "        \"Saving completed!\"\n",
    "        \n",
    "    return x_t,y_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_switch(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,svrg_stepsize,initial_Kmax,\n",
    "                      sgd_epsilon,Tsvrg,threshold):\n",
    "    \n",
    "    \"T0 = number of iterations using SG oracle to approximate Phi_0 \"\n",
    "    \"We replace xstar, ystar in Phi0 by xT0, yT0\"\n",
    "    \n",
    "    old_Kmax = initial_Kmax ## setting up T0\n",
    "    \n",
    "    \n",
    "    \"calling IPDHG with SG oracle for T0 = initial_Kmax iterations\"\n",
    "\n",
    "    x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt,Hw_yt,full_sgd_distances = compressed_SGD(A,b,x0,y0,D_x,D_y,H_x,H_y,\n",
    "                                                                       Hw_x,Hw_y,sgd_stepsize,\n",
    "                                                                           initial_Kmax)\n",
    "    \n",
    "    \"accessing local distances between last two consecutive iterations and their average\"\n",
    "    \n",
    "    full_sgd_dist_local_zt_z0 = full_sgd_distances['full_sgd_loc_dist']\n",
    "    avg_full_sgd_dist = full_sgd_distances['full_sgd_avg_dist']\n",
    "    \n",
    "\n",
    "    print ('avg_full_sgd_dist',avg_full_sgd_dist)\n",
    "    \n",
    "    \"Continue with SG oracle if difference between last two iterates xT0, xT0-1 is not very small\"\n",
    "\n",
    "    print ('Gap between two last iterates is sufficient?',np.any(avg_full_sgd_dist > threshold))\n",
    "#     print (np.any(avg_full_sgd_dist > threshold) == True)\n",
    "    if (np.any(avg_full_sgd_dist > threshold) == True):\n",
    "        print ('Then continue with SG oracle!!')\n",
    "        \n",
    "        new_epsilon = best_epsilon0(target_acc,delta,initial_x0,initial_y0,x_t,y_t,sgd_stepsize,\n",
    "                                        features,values,W)\n",
    "        \n",
    "        print ('new epsilon0',new_epsilon)\n",
    "\n",
    "        \"Computing T_{j+1}\"\n",
    "\n",
    "        new_Kmax = sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,new_epsilon)\n",
    "\n",
    "        print ('new Kmax',new_Kmax)\n",
    "        print ('old Kmax',old_Kmax)\n",
    "\n",
    "        \"Computing T{j+1}- T_{j} \"\n",
    "        Kmax = new_Kmax - old_Kmax ## T{j+1}- T_{j}\n",
    "        print ('Iterations Tj+1,Tj,Tj+1 - Tj', [new_Kmax,old_Kmax,Kmax])\n",
    "\n",
    "        old_Kmax = new_Kmax\n",
    "#         Kmax = 100 ## to check subsequent steps\n",
    "        \n",
    "        \"Perform Kmax = T{j+1}- T_{j} iterations using SG oracle\"\n",
    "        x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t, Hw_xt, Hw_yt,sgd_distances = compressed_SGD(A,b,\n",
    "                                                                                x_t,y_t,Dx_t,Dy_t,\n",
    "                                                                                Hx_t,Hy_t,Hw_xt,\n",
    "                                                                                Hw_yt,sgd_stepsize,Kmax)\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    \"switch to IPDHG with svrg oracle\"\n",
    "    \n",
    "    \n",
    "    x_t, y_t = compressed_SVRG(A,b,x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt, Hw_yt,svrg_stepsize,Tsvrg)\n",
    "                                       \n",
    "    \n",
    "    \n",
    "    return x_t,y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_phi0(Mx,My,gamma_x,gamma_y,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W):\n",
    "    \n",
    "    \"Mx, My, gamma_x, gamma_y, alpha_x, alpha_y are the parameters associated with sgd oracle\"\n",
    "    \"zT0 = (xT0,yT0) are the last iterates of IPDHG with SG oracle\"\n",
    "    \n",
    "    I = np.identity(nodes)\n",
    "    I_W = np.subtract(I,W)\n",
    "\n",
    "    local_phi0 = np.zeros(nodes)\n",
    "    \n",
    "    norm_x0_xT0 = np.zeros(nodes)\n",
    "    norm_y0_yT0 = np.zeros(nodes)\n",
    "\n",
    "    \"store full batch gradients at f_i(zT0)\"\n",
    "    \n",
    "    FB_grad_x = np.zeros((nodes,dimension_x))\n",
    "    FB_grad_y = np.zeros((nodes,dimension_y))\n",
    "    \n",
    "    \"stores difference of grad f_i and average (over nodes) gradients f_i to obtain (i-J)grad F \"\n",
    "    \n",
    "    grad_diff_x = np.zeros((nodes,dimension_x))\n",
    "    grad_diff_y = np.zeros((nodes,dimension_y))\n",
    "    \n",
    "    norm_grad_diff_x = np.zeros(nodes) ## store norm of local Dxstar\n",
    "    norm_grad_diff_y = np.zeros(nodes) ## store norm of local Dystar\n",
    "    \n",
    "    \n",
    "    for i in range(nodes):\n",
    "        \"computing local full batch gradients\"\n",
    "        FB_grad_x[i], FB_grad_y[i] = full_batch_gradient(features[i],values[i],regcoef_x,scaling_factor,positive_samples,xT0[i],yT0[i])\n",
    "        \n",
    "        \n",
    "        \"computing ||x0-xT0||^2\"\n",
    "    \n",
    "        diff_x0_xT0 = np.subtract(x0[i],xT0[i])\n",
    "        norm_x0_xT0[i] = (LA.norm(diff_x0_xT0))**2\n",
    "    #     Mx_xstar = Mx*nodes*norm_x0_xstar\n",
    "\n",
    "        \"computing ||y0-yT0||^2\"\n",
    "\n",
    "        diff_y0_yT0 = np.subtract(y0[i],yT0[i])\n",
    "        norm_y0_yT0[i] = (LA.norm(diff_y0_yT0))**2\n",
    "\n",
    "    avg_grad_x = acce_consensus_ite(W,nodes,eta,tau,FB_grad_x) ## average of grad_x f_i's\n",
    "    avg_grad_y = acce_consensus_ite(W,nodes,eta,tau,FB_grad_y) ## average of grad_y f_i 's'\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(nodes):\n",
    "        grad_diff_x[i] = np.subtract(FB_grad_x[i],avg_grad_x[i]) ## approx Dxstar\n",
    "        grad_diff_y[i] = np.subtract(FB_grad_y[i],avg_grad_y[i]) ## approx Dystar\n",
    "        \n",
    "        norm_grad_diff_x[i] = (LA.norm(grad_diff_x[i]))**2\n",
    "        norm_grad_diff_y[i] = (LA.norm(grad_diff_y[i]))**2\n",
    "        \n",
    "    \n",
    "    \"computing square norm of H0x - Hstar_x and H0y - Hstar_y\"\n",
    "\n",
    "    diff_H0x_Hstar_x =  np.add(diff_x0_xT0, sgd_stepsize*avg_grad_x) \n",
    "    diff_H0y_Hstar_y =  np.subtract(diff_y0_yT0, sgd_stepsize*avg_grad_y)\n",
    "    \n",
    "    norm_diff_H0x_Hstar_x = (LA.norm(diff_H0x_Hstar_x,axis = 1))**2\n",
    "    norm_diff_H0y_Hstar_y = (LA.norm(diff_H0y_Hstar_y,axis = 1))**2\n",
    "    \n",
    "    for i in range(nodes):\n",
    "        local_phi0[i] = Mx*norm_x0_xT0[i] + My*norm_y0_yT0[i] + (2*sgd_stepsize**2)/(gamma_x*lambda_second_small)*norm_grad_diff_x[i] + (2*sgd_stepsize**2)/(gamma_y*lambda_second_small)*norm_grad_diff_y[i] + math.sqrt(delta)*(norm_diff_H0x_Hstar_x[i] + norm_diff_H0y_Hstar_y[i])\n",
    "    \n",
    "\n",
    "    phi_0 = acce_consensus(W,nodes,eta,tau,local_phi0)\n",
    "    \n",
    "    return phi_0,(LA.norm(avg_grad_x))**2, (LA.norm(avg_grad_y))**2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"best epsilon0 using approximation of phi0\"\n",
    "\n",
    "def best_epsilon0(target_acc,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W):\n",
    "    \n",
    "    \"parameters for svrg\"\n",
    "    \n",
    "    tilde_cx, tilde_cy, svrg_bx, svrg_by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
    "\n",
    "\n",
    "    svrg_alpha_x, svrg_alpha_y, svrg_gamma_x, svrg_gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(\n",
    "                                                            mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W)\n",
    "\n",
    "    svrg_Mx, svrg_My = MxMy(svrg_alpha_x, svrg_alpha_y, svrg_gamma_x, svrg_gamma_y,lambda_max,delta)\n",
    "    \n",
    "    \"Computes Phi0\"\n",
    "    \n",
    "    \n",
    "    sgd_bx, sgd_by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize) ## sgd param\n",
    "    sgd_alpha_x, sgd_alpha_y, sgd_gamma_x, sgd_gamma_y, sgd_Mx, sgd_My = sgd_parameters_alpha_gammaMxMy(sgd_bx,sgd_by,mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max)\n",
    "\n",
    "#     computing_phi0(Mx,My,gamma_x,gamma_y,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W)\n",
    "    avg_phi_0,norm_avg_grad_x,norm_avg_grad_y = computing_phi0(sgd_Mx,sgd_My,sgd_gamma_x,sgd_gamma_y,delta,x0,y0,\n",
    "                                                 xT0,yT0,sgd_stepsize,features,values,W)\n",
    "    \n",
    "    print ('Local phi0',avg_phi_0)\n",
    "    phi_0 = max(avg_phi_0)\n",
    "    svrg_rho = rho_svrg(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize,W)\n",
    "\n",
    "    \"Computes Cmax\"\n",
    "    \n",
    "    first_term  = (svrg_Mx+tilde_cx)/sgd_Mx\n",
    "    sec_term = (svrg_My+tilde_cy)/sgd_My\n",
    "    third_term = (sgd_gamma_x*svrg_stepsize**2)/(svrg_gamma_x*sgd_stepsize**2)\n",
    "    fourth_term = (sgd_gamma_y*svrg_stepsize**2)/(svrg_gamma_y*sgd_stepsize**2)\n",
    "\n",
    "    \n",
    "    Cmax = max( first_term,sec_term,third_term,fourth_term, 2 )\n",
    "    print ('Cmax:',Cmax)\n",
    "    epsilon0 = target_acc/(2*Cmax*phi_0)\n",
    "    \n",
    "    print ('best epsilon0',epsilon0)\n",
    "    \n",
    "    return epsilon0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing parameters for sgd oracle\n",
    "def sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize):\n",
    "\n",
    "    bx = sgd_stepsize*mux - 4*sgd_stepsize**2*Lyx**2 \n",
    "    by = sgd_stepsize*muy - 4*sgd_stepsize**2*Lxy**2 \n",
    " \n",
    "    return bx, by\n",
    "\n",
    "def sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max):\n",
    "    \n",
    "    alpha_x = bx/(1+delta)\n",
    "    alpha_y = by/(1+delta)\n",
    "    \n",
    "    \"gamma_x\"\n",
    "    gamma_x_second = 1/(4*(1+delta)*lambda_max)\n",
    "    gamma_x_first = gamma_x_second*(bx/math.sqrt(delta))\n",
    "    gamma_x = min(gamma_x_first, gamma_x_second)\n",
    "    \n",
    "    \"gamma_y\"\n",
    "    gamma_y_first = gamma_x_second*(by/math.sqrt(delta))\n",
    "    gamma_y = min(gamma_y_first,gamma_x_second)\n",
    "    \n",
    "    \"Mx,My\"\n",
    "    \n",
    "    Mx = 1 - (math.sqrt(delta)*alpha_x)/(1-0.5*gamma_x*lambda_max)\n",
    "    My = 1 - (math.sqrt(delta)*alpha_y)/(1-0.5*gamma_y*lambda_max)\n",
    "\n",
    "    return alpha_x, alpha_y, gamma_x, gamma_y, Mx, My\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,sgd_epsilon):\n",
    "\n",
    "    bx,by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize)\n",
    "    alpha_x, alpha_y, gamma_x, gamma_y, Mx, My = sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,\n",
    "                                                            Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max)\n",
    "    \n",
    "    T1 = (1-bx)/Mx\n",
    "    T2 = (1-by)/My\n",
    "    T3 = 1 - 0.5*gamma_x*lambda_second_small\n",
    "    T4 = 1 - 0.5*gamma_y*lambda_second_small\n",
    "    T5 = 1-alpha_x\n",
    "    T6 = 1-alpha_y\n",
    "    rho = max(T1,T2,T3,T4,T5,T6)\n",
    "    print ('sgd rho:',rho)\n",
    "    denominator = (-1)*math.log(rho)\n",
    "    T = (-1*math.log(sgd_epsilon))/denominator\n",
    "#     print ('calls of SGD oracle',int(T))\n",
    "    return int(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing parameters like step size, tilde_cx, tilde_cy, bx and by\n",
    "def parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize):\n",
    "    mu = min(mux,muy)\n",
    "    L = max(Lxx,Lyy,Lxy,Lyx)\n",
    "#     print ('L',L)\n",
    "#     step = mu/(21*L**2)\n",
    "    num_tilde_cx = 8*svrg_stepsize**2*(Lxx**2 + Lyx**2)\n",
    "    tilde_cx = num_tilde_cx/ref_prob ## Taking uniform sample distribution\n",
    "    num_tilde_cy = 8*svrg_stepsize**2*(Lyy**2 + Lxy**2)\n",
    "    tilde_cy = num_tilde_cy/ref_prob \n",
    "    bx = svrg_stepsize*mux - 4*svrg_stepsize**2*Lyx**2 - tilde_cx*ref_prob\n",
    "    by = svrg_stepsize*muy - 4*svrg_stepsize**2*Lxy**2 - tilde_cy*ref_prob\n",
    "    \n",
    "    return tilde_cx, tilde_cy, bx, by\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_alpha_gamma(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W):\n",
    "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
    "    alpha_x = bx/(1+delta)\n",
    "    alpha_y = by/(1+delta)\n",
    "    I = np.identity(nodes)\n",
    "    I_W = np.subtract(I,W)\n",
    "    eigvalues , eigvectors = eig(I_W)\n",
    "    eigvalues = np.sort(eigvalues) ## sort eigenvalues in increasing order\n",
    "    lambda_max = eigvalues[-1]\n",
    "    lambda_second_small = eigvalues[1]\n",
    "    \"gamma_x\"\n",
    "    gamma_x_second = 1/(4*(1+delta)*lambda_max)\n",
    "    gamma_x_first = gamma_x_second*(bx/math.sqrt(delta))\n",
    "    gamma_x = min(gamma_x_first, gamma_x_second)\n",
    "    \n",
    "    \"gamma_y\"\n",
    "    gamma_y_first = gamma_x_second*(by/math.sqrt(delta))\n",
    "    gamma_y = min(gamma_y_first,gamma_x_second)\n",
    "    \n",
    "\n",
    "    return alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta):\n",
    "    \"Mx and My\"\n",
    "    Mx = 1 - (math.sqrt(delta)*alpha_x)/(1-0.5*gamma_x*lambda_max)\n",
    "    My = 1 - (math.sqrt(delta)*alpha_y)/(1-0.5*gamma_y*lambda_max)\n",
    "    return Mx,My\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_svrg(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize,W):\n",
    "    \n",
    "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
    "    alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W)\n",
    "    Mx,My = MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta)\n",
    "    \n",
    "    T1 = (1-bx)/Mx\n",
    "    T2 = (1-by)/My\n",
    "    T3 = 1 - 0.5*gamma_x*lambda_second_small\n",
    "    T4 = 1 - 0.5*gamma_y*lambda_second_small\n",
    "    T5 = 1-alpha_x\n",
    "    T6 = 1-alpha_y\n",
    "    rho = max(T1,T2,T3,T4,T5,T6)\n",
    "    print ('svrg rho',rho)\n",
    "    \n",
    "    return rho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_graph(n=0,create_using=None):\n",
    "    \"\"\"Return the empty graph with n nodes and zero edges.\n",
    "\n",
    "    Node labels are the integers 0 to n-1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if create_using is None:\n",
    "        # default empty graph is a simple graph\n",
    "        G=nx.Graph()\n",
    "    else:\n",
    "        G=create_using\n",
    "        G.clear()\n",
    "\n",
    "    G.add_nodes_from(range(n))\n",
    "    G.name=\"empty_graph(%d)\"%n\n",
    "    return G\n",
    "\n",
    "\n",
    "def grid_2d(m,n,periodic=False,create_using=None): ## m,n be the number of rows and number of \n",
    "    # columns in torus topolgy\n",
    "    \n",
    "    \"\"\" Return the 2d grid graph of mxn nodes,\n",
    "        each connected to its nearest neighbors.\n",
    "        Optional argument periodic=True will connect\n",
    "        boundary nodes via periodic boundary conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    G=empty_graph(0,create_using)\n",
    "    G.name=\"grid_2d_graph\"\n",
    "    rows=range(m)\n",
    "    columns=range(n)\n",
    "    G.add_nodes_from( (i,j) for i in rows for j in columns )\n",
    "    G.add_edges_from( ((i,j),(i-1,j)) for i in rows for j in columns if i>0 )\n",
    "    G.add_edges_from( ((i,j),(i,j-1)) for i in rows for j in columns if j>0 )\n",
    "    if G.is_directed():\n",
    "        G.add_edges_from( ((i,j),(i+1,j)) for i in rows for j in columns if i<m-1 )\n",
    "        G.add_edges_from( ((i,j),(i,j+1)) for i in rows for j in columns if j<n-1 )\n",
    "    if periodic:\n",
    "        if n>2:\n",
    "            G.add_edges_from( ((i,0),(i,n-1)) for i in rows )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((i,n-1),(i,0)) for i in rows )\n",
    "        if m>2:\n",
    "            G.add_edges_from( ((0,j),(m-1,j)) for j in columns )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((m-1,j),(0,j)) for j in columns )\n",
    "        G.name=\"periodic_grid_2d_graph(%d,%d)\"%(m,n)\n",
    "    return G\n",
    "\n",
    "\n",
    "def gen_graph(row,column,m):\n",
    "    W = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
    "    \n",
    "    \"Generating 2D Grid\"\n",
    "    G = grid_2d(row,column,periodic=False,create_using=None)\n",
    "    \n",
    "#     \"Adding extra edges to get 2D Torus\"\n",
    "#     edges_rows = [((0,0),(0,4)),((1,0),(1,4)),((2,0),(2,4)),((3,0),(3,4))] \n",
    "#     column_rows = [((0,0),(3,0)),((0,1),(3,1)),((0,2),(3,2)),((0,3),(3,3)),((0,4),(3,4))] \n",
    "#     G.add_edges_from(edges_rows)\n",
    "#     G.add_edges_from(column_rows)\n",
    "#     nx.draw_networkx(G)\n",
    "    plt.savefig('2D-Grid')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    \"Changing edges format to 1D so that it becomes easy to access the indices\"\n",
    "    \n",
    "    edges = []\n",
    "    for i in range(row):\n",
    "        for j in range(column-1):\n",
    "            edges.append([j+i*column,j+1+i*column])\n",
    "\n",
    "    for i in range(row-1):\n",
    "        for j in range(column):\n",
    "            edges.append([j+i*column,j+(i+1)*column])      \n",
    "\n",
    "    \"Adding extra edges to get 2D Torus from 2D grid\"\n",
    "\n",
    "    for i in range(column):\n",
    "        edges.append([i , i + (row-1)*column])\n",
    "\n",
    "    for i in range(row):\n",
    "        edges.append([i*column , row + i*column]) \n",
    "\n",
    "#     print ('edges:',edges)   \n",
    "    print ('total edges in 2D Torus:',len(edges))\n",
    "    \n",
    "    for (u, v) in edges:\n",
    "        W[u][v] = 1/5\n",
    "        W[v][u] = 1/5\n",
    "\n",
    "    \n",
    "    for i in range(m):\n",
    "        W[i][i] = 1/5\n",
    "\n",
    "    \n",
    "    B = np.matrix(W)\n",
    "    print ((B.transpose() == W).all()) ## check symmetric property of B. It will print True\n",
    "    print ([sum(W[i]) for i in range(m)])\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def auc_curve(A_dense,data_labels,w):\n",
    "    \n",
    "    score = np.zeros(A_dense.shape[0])\n",
    "    for i in range(A_dense.shape[0]):\n",
    "        score[i] = np.dot(w,A_dense[i])\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(data_labels, score, pos_label=1)\n",
    "#     print ('fpr =',fpr)\n",
    "#     print ('tpr =',tpr)\n",
    "    auc_metric = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    return auc_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Setting up the constant parameters\"\n",
    "\n",
    "\"nodes and number of batches\"\n",
    "row = 4 ## row >=3\n",
    "column = 5 ## column = row+1\n",
    "nodes = row*column ### total number of nodes\n",
    "num_batches = 20 ## This must be fixed for comparison\n",
    "\n",
    "\"regularization parameter and constraint set diameter\"\n",
    "\n",
    "regcoef_x = 10**(-5) ## lambda\n",
    "radius_x = 100\n",
    "radius_y = 2*radius_x\n",
    "\n",
    "num_bits = 4\n",
    "scaling_factor = A.shape[0]\n",
    "primal_dim = A.shape[1]+2\n",
    "dimension_x = primal_dim\n",
    "dimension_y = 1\n",
    "\n",
    "sample_prob = (1/num_batches)*np.ones(num_batches)\n",
    "print ('sample prob',sample_prob)\n",
    "\n",
    "ref_prob = 1/num_batches\n",
    "\n",
    "print ('ref prob',ref_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding delta\n",
    "\n",
    "\n",
    "delta_x = delta_qsgd(num_bits,primal_dim)\n",
    "delta_y = delta_qsgd(num_bits,dimension_y)\n",
    "delta = max(delta_x,delta_y)\n",
    "print ('delta',delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\" finding mux, muy, fraction of positive examples\"\n",
    "\n",
    "positive_samples = (np.count_nonzero(b==1))/b.shape[0]\n",
    "print ('positive_samples =',positive_samples)\n",
    "\n",
    "min_local_samples = math.floor(A.shape[0]/nodes)\n",
    "\n",
    "mux = (2*min(positive_samples,1-positive_samples)*min_local_samples)/A.shape[0] + regcoef_x/nodes\n",
    "muy = (2*positive_samples*(1-positive_samples)*min_local_samples)/A.shape[0]\n",
    "\n",
    "mu = min(mux,muy)\n",
    "print ('mu',mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"generate weight matrix\"\n",
    "\n",
    "W = gen_graph(row,column,nodes)\n",
    "\n",
    "eigenvalues_W, eigenvectors_W = eig(W)\n",
    "sorted_eigenvalues_W = np.sort(eigenvalues_W)\n",
    "print ('eigenvalues of W', sorted_eigenvalues_W)\n",
    "\n",
    "\n",
    "\"maximum and second smallest eigenvalue of I-W\"\n",
    "\n",
    "I = np.identity(nodes)\n",
    "I_W = np.subtract(I,W)\n",
    "eigvalues , eigvectors = eig(I_W)\n",
    "eigvalues = np.sort(eigvalues) ## sort eigenvalues in increasing order\n",
    "lambda_max = eigvalues[-1]\n",
    "lambda_second_small = eigvalues[1]\n",
    "\n",
    "\"constants eta and tau used in accelerated consensus method \"\n",
    "\n",
    "muW = sorted_eigenvalues_W[nodes-2] ## second largest eigen value of W\n",
    "print ('muW',muW)\n",
    "eta = (1 - math.sqrt(1 - muW**2))/(1 + math.sqrt(1 - muW**2))\n",
    "tau = 20 ## iterations in accelerated consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribute data point among nodes\n",
    "features, values = data_blocks(A,b,nodes)\n",
    "\n",
    "## creating minibatches for all nodes\n",
    "\n",
    "mini_batch_features,mini_batch_values = nodes_mini_batches(features,values,nodes,num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing Lipschitz parameter\n",
    "Lxx,Lyy,Lxy,Lyx = global_lipschitz(mini_batch_features,mini_batch_values,nodes\n",
    "                    ,num_batches,regcoef_x,scaling_factor,positive_samples)\n",
    "\n",
    "print ('global Lipschitz parameters',(Lxx,Lyy,Lxy,Lyx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Initializations\"\n",
    "\n",
    "L = max(Lxx,Lyy,Lxy,Lyx)\n",
    "mu = min(mux,muy)\n",
    "\n",
    "\"stepsize used in SG oracle\"\n",
    "\n",
    "sgd_stepsize = mu/(4*L**2) ## step size for sgd\n",
    "print ('sgd_stepsize',sgd_stepsize)\n",
    "\n",
    "\"step size used in svrg oracle\"\n",
    "\n",
    "svrg_stepsize = mu/(21*L**2)\n",
    "print ('svrg step size',svrg_stepsize)\n",
    "\n",
    "\"Initial Kmax\"\n",
    "\n",
    "sgd_epsilon = 0.5\n",
    "\n",
    "initial_Kmax = sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,sgd_epsilon)\n",
    "\n",
    "# initial_Kmax = 10\n",
    "print ('sgd_iterations =',initial_Kmax)\n",
    "\n",
    "\n",
    "# Tcheck = 100 ## small iterations to decide switching\n",
    "Tsvrg = 10**8 ## number of iterations of IPDHG + SVRG\n",
    "\n",
    "threshold = 1e-08\n",
    "target_acc = 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating files to save output\n",
    "\n",
    "AUC_val = open(r\"AUC_val_sgd_switch_svrg_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "sum_consensus_error_x = open(r\"sgd_switch_tosvrg_consensus_error_x_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "sum_consensus_error_y = open(r\"sgd_switch_tosvrg_consensus_error_y_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "total_distance_from_saddle = open(r\"sgd_switch_tosvrg_total_distance_from_saddle_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "function_values = open(r\"sgd_switch_tosvrg_function_values_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "full_batch_grad_counts = open(r\"sgd_switch_tosvrg_full_batch_grad_counts_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "compression_error_nux = open(r\"sgd_switch_tosvrg_compression_error_nux_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "compression_error_nuy = open(r\"sgd_switch_tosvrg_compression_error_nuy_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
    "\n",
    "\n",
    "x0 = np.zeros((nodes,primal_dim))\n",
    "y0 = np.zeros((nodes,dimension_y))\n",
    "\n",
    "print ('shape of x0',x0.shape)\n",
    "print ('shape of y0',y0.shape)\n",
    "\n",
    "\n",
    "H_x = np.copy(x0)\n",
    "H_y = np.copy(y0)\n",
    "\n",
    "D_x = np.copy(x0)\n",
    "D_y = np.copy(y0)\n",
    "\n",
    "Hw_x = oneConsensus(W,nodes,H_x)\n",
    "Hw_y = oneConsensus(W,nodes,H_y)\n",
    "    \n",
    "initial_x0 = np.copy(x0)\n",
    "initial_y0 = np.copy(y0)\n",
    "\n",
    "## Loading saddle point solution (xstar, ystar). \n",
    "## Change the file name inside np.loadtxt('file_name.txt') accordingly\n",
    "\n",
    "xstar = np.loadtxt('xstar_a4a_lambda_10_5_Tint_4385060.txt')\n",
    "ystar_scalar = np.loadtxt('ystar_a4a_lambda_10_5_Tint_4385060.txt')\n",
    "ystar = np.array([ystar_scalar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xT, yT = heuristic_switch(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,svrg_stepsize,initial_Kmax,\n",
    "                      sgd_epsilon,Tsvrg,threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
