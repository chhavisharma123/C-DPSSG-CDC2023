{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Chhavi Sharma\n",
        "\n",
        "Affiliation: Industrial Engineering and Operations Research\n",
        "\n",
        "Email: chhavisharma@iitb.ac.in\n",
        "\n",
        "Notebook Description: This notebook implements C-DPSSG algorithm to solve robust logistic regression problem\n",
        "\\begin{align}\n",
        "\t\\min_{ x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} \\Psi(x,y) & =  \\frac{1}{N} \\sum_{i = 1}^N \\log\\left( 1+ \\exp\\left( -b_ix^\\top(a_i + y)\\right) \\right)  + \\frac{\\lambda}{2} \\left\\Vert x \\right\\Vert^2_2 -\\frac{\\beta}{2} \\left\\Vert y \\right\\Vert^2_2\n",
        "\\end{align}\n",
        "over a binary classification data set $\\mathcal{D} = \\{(a_i, b_i) \\}_{i = 1}^N$.  The constraint sets $\\mathcal{X}$ and $\\mathcal{Y}$ are $\\ell_2$ balls of radius $100$ and $1$ respectively."
      ],
      "metadata": {
        "id": "zr3OG14ztldp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing required packages"
      ],
      "metadata": {
        "id": "JqiH5wMvuyse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kl6MrFltIGg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random as rd\n",
        "import networkx as nx\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import linalg as LA\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from random import choice\n",
        "from scipy.stats import bernoulli\n",
        "from numpy.linalg import eig\n",
        "import scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQsBcwPTtIGk"
      },
      "source": [
        "# Loading data set\n",
        "To download the data, [click here](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd6SN-zvtIGm"
      },
      "outputs": [],
      "source": [
        "data_features, data_labels = load_svmlight_file(\"a4a.txt\")\n",
        "\"\"\"\n",
        "converting sparse matrix myfeature (which is in tuple form) to a dense matrix\n",
        "\"\"\"\n",
        "A_dense = data_features.todense()\n",
        "\n",
        "A_dense = np.array(A_dense)\n",
        "\n",
        "print (type(A_dense))\n",
        "print (type(data_labels))\n",
        "\n",
        "num_samples = A_dense.shape[0]\n",
        "num_features = A_dense.shape[1]\n",
        "print (data_labels)\n",
        "print('num samples: %d num features : %d ' %(num_samples, num_features))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WHVyCq7tIGn"
      },
      "source": [
        "# Shuffling the data indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvdXQNJutIGo"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "indices = np.arange(num_samples)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "A = A_dense[indices]\n",
        "b = data_labels[indices]\n",
        "\n",
        "\n",
        "print ('shape of A',A.shape)\n",
        "print ('shape of b',b.shape)\n",
        "\n",
        "print ('Minimum feature value =',np.min(A))\n",
        "print ('Maximum feature value =',np.max(A))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nx1-NeBtIGp"
      },
      "source": [
        "# Distribute data points among nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UIR1y45tIGp"
      },
      "outputs": [],
      "source": [
        "def data_blocks(A,b,m):\n",
        "    \"\"\"\n",
        "    This function distributes features and corresponding lables to m nodes\n",
        "\n",
        "    Input\n",
        "    ---------\n",
        "     A: feature matrix, b: labels, m: number of nodes\"\n",
        "\n",
        "     Returns\n",
        "     --------\n",
        "    features: A list containing features of all nodes\n",
        "    values: A list containing associated labels of all nodes\n",
        "\n",
        "    \"\"\"\n",
        "    features = [[] for i in range(m)]\n",
        "    values = [[] for i in range(m)]\n",
        "    N = A.shape[0] ## N = number of samples\n",
        "    indices = np.arange(N)\n",
        "    size = int(N/m)  ## size of each block\n",
        "    for i in range(m):\n",
        "        start_idx = i*size\n",
        "        end_idx = min(N,(i+1)*size)\n",
        "        features[i] = A[indices[start_idx:end_idx]]\n",
        "        values[i] = b[indices[start_idx:end_idx]]\n",
        "\n",
        "    samples_after_eq_dist = m*int(N/m)  ## total samples after equally distributed samples to every node\n",
        "    remaining_samples = N - m*int(N/m) ## This quantity will always be less than m\n",
        "    if ( remaining_samples >= 1):\n",
        "        for j in range(remaining_samples):\n",
        "            features[j] = np.vstack((features[j],A[samples_after_eq_dist + j]))\n",
        "            values[j] = np.hstack((values[j],b[samples_after_eq_dist + j]))\n",
        "\n",
        "    return features,values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create minibatches of local dataset/node's datapoints"
      ],
      "metadata": {
        "id": "y9s03vWbxxBf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE5y3HRwtIGq"
      },
      "outputs": [],
      "source": [
        "## creating n mini-batches of local data points\n",
        "def create_mini_batches(local_features,local_values,num_batches):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    -------\n",
        "    local_features: list of features of a particular node\n",
        "    local_values: list of corresponding labels\n",
        "    num_batches: number of mini-batches to be created\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    batch_features: minibatches of features\n",
        "    batch_values: minibatches of associated labels\n",
        "    \"\"\"\n",
        "\n",
        "    batch_features = [[] for i in range(num_batches)]\n",
        "    batch_values = [[] for i in range(num_batches)]\n",
        "    samples = len(local_features)\n",
        "    indices = np.arange(samples)\n",
        "    batch_size = int(samples/num_batches)\n",
        "    for batch in range(num_batches):\n",
        "        start_idx = batch*batch_size\n",
        "        end_idx = min(samples,(batch+1)*batch_size)\n",
        "        batch_features[batch] = local_features[start_idx:end_idx]\n",
        "        batch_values[batch] = local_values[start_idx:end_idx]\n",
        "    samples_after_eq_dist = num_batches*int(samples/num_batches)\n",
        "    remaining_samples = samples - samples_after_eq_dist ## This quantity will always be less than m\n",
        "    if ( remaining_samples >= 1):\n",
        "        for j in range(remaining_samples):\n",
        "            batch_features[j] = np.vstack((batch_features[j],local_features[samples_after_eq_dist+j])) ## remove indices\n",
        "            batch_values[j] = np.hstack((batch_values[j],local_values[samples_after_eq_dist+j]))\n",
        "\n",
        "    return batch_features,batch_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create minibatches of all nodes using previous function \"create_mini_batches\""
      ],
      "metadata": {
        "id": "zcQ9UdJ9qDeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI-75WtqtIGr"
      },
      "outputs": [],
      "source": [
        "def nodes_mini_batches(features,values,nodes,num_batches):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    features, values: local datasets of all nodes\n",
        "    num_batches: number of batches at each node\n",
        "\n",
        "    Returns:\n",
        "    mini_batch_features: List containing minibatch features of all nodes\n",
        "    mini_batch_values: List containing minibatch labels of all nodes\n",
        "    \"\"\"\n",
        "\n",
        "    mini_batch_features = [[] for i in range(nodes)]\n",
        "    mini_batch_values = [[] for i in range(nodes)]\n",
        "    shuffled_features = [[] for i in range(nodes)]\n",
        "    shuffled_values = [[] for i in range(nodes)]\n",
        "    for i in range(nodes):\n",
        "        \"shuffling local samples befor creating mini-batches\"\n",
        "        local_num_samples = features[i].shape[0]\n",
        "        indices = np.arange(local_num_samples)\n",
        "        np.random.seed(i+10) ## For reproducimg same minibatches for comparative algorithms\n",
        "        np.random.shuffle(indices)\n",
        "        shuffled_features[i] = features[i][indices]\n",
        "        shuffled_values[i] = values[i][indices]\n",
        "        mini_batch_features[i],mini_batch_values[i] = create_mini_batches(shuffled_features[i]\n",
        "                                                                          ,shuffled_values[i],\n",
        "                                                                        num_batches)\n",
        "\n",
        "    return mini_batch_features, mini_batch_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finding robust logistic regression function value at point $(x,y)$"
      ],
      "metadata": {
        "id": "21_QWq6y1wWh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3qK_GndtIGs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def logistic_func(A,b,x,y,nodes,num_batches,regcoef_x,regcoef_y,scaling_factor):\n",
        "    \"\"\"\n",
        "    logistic function value computed at (x,y)\n",
        "    Input:\n",
        "    Features: A\n",
        "    Labels: b\n",
        "    nodes: number of nodes\n",
        "    regcoef_x,regcoef_y: lambda, beta\n",
        "    scaling_factor: Total number of samples (N)\n",
        "    (x,y): Point at which function value is computed\n",
        "\n",
        "    Returns:\n",
        "    Robust logistic regression function value at (x,y) with dataset (A,b)\n",
        "    \"\"\"\n",
        "\n",
        "    func = 0\n",
        "    N = A.shape[0] ## number of samples\n",
        "    for i in range(N):\n",
        "        perturbed_feature = np.add(A[i],y)\n",
        "        product = np.dot(x,perturbed_feature)\n",
        "        domain = b[i]*product\n",
        "        if (domain > 0):\n",
        "            num = 1+ math.exp((-1)*domain)\n",
        "            log_num = math.log(num)\n",
        "            func = func + log_num\n",
        "        else:\n",
        "            num = 1+ math.exp(domain)\n",
        "            log_num = (-1)*domain + math.log(num)\n",
        "            func = func + log_num\n",
        "\n",
        "\n",
        "    function = func/scaling_factor + 0.5*regcoef_x*(LA.norm(x))**2 - 0.5*regcoef_y*(LA.norm(y))**2\n",
        "\n",
        "    return function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find Lipschitz parameters of function components $f_{ij}$.\n",
        "Recall that local function $f_i$ is given as $f_i(x,y) = \\frac{1}{n}\\sum_{j=1}^n f_{ij}(x,y)$, where $n$ is the number of batches at node $i$. Lipschitz constants for the gradients of logistic regression function are given as\n",
        "\\begin{align}\n",
        "& L^{ij}_{xx} = \\frac{n}{2N} \\sum_{l = 1}^{N_{ij}} \\Vert a^j_{il} \\Vert_2^2 + \\frac{nN_{ij}R^2_y}{2N} + \\frac{\\lambda}{m}, \\\\\n",
        "& L^{ij}_{yy} = \\frac{nN_{ij}R_x^2}{4N} + \\frac{\\beta}{m},\\\\\n",
        "& L^{ij}_{xy} = \\frac{n}{N} \\left( \\left(1+ \\frac{R_xR_y}{4} \\right)N_{ij} + \\frac{R_x}{4}  \\sum_{l = 1}^{N_{ij}} \\Vert a^j_{il} \\Vert_2 \\right),\n",
        "\\end{align}\n",
        "where\n",
        "\\begin{align}\n",
        "&N = \\text{number of samples},\\\\\n",
        "&n = \\text{number of minibatches}, \\\\\n",
        "& R_x, R_y = \\text{Radius of constraint sets} \\ \\mathcal{X} \\ \\text{and} \\ \\mathcal{Y}, \\\\\n",
        "&N_{ij} = \\text{number of samples in} \\ j\\text{-th minibatch of node } \\ i,\\\\\n",
        "&a^j_{il} =  l\\text{-th feature of} \\ j\\text{-th minibatch at node } \\ i\n",
        "\\end{align}\n",
        "Then, set $L_{xx} = \\max_{i,j} \\{ L^{ij}_{xx} \\}, \\ L_{yy} = \\max_{i,j} \\{L^{ij}_{yy}\\}$ and $L_{xy} = L_{yx} = \\max_{i,j} \\{L^{ij}_{xy} \\}$."
      ],
      "metadata": {
        "id": "JpxpobGOqXFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUsXbOD0tIGr"
      },
      "outputs": [],
      "source": [
        "## Finding Lipschitz parameters for f_ij(x,y)\n",
        "def local_lipschitz_constants(batch_features,batch_values,radius_x,radius_y,regcoef_x,regcoef_y\n",
        "                                 ,scaling_factor):\n",
        "\n",
        "    \"\"\"\n",
        "    This function returns the Lipschitz parameters L^{ij}_xx, L^{ij}_yy,\n",
        "    L^{ij}_xy and L^{ij}_yx\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    number_samples = len(batch_features) ## batch size\n",
        "    A_norm = LA.norm(batch_features ,2)\n",
        "    Lxx_tilde = 0.25*(2*A_norm**2 + 2*number_samples*radius_y**2)\n",
        "    Lxx = (num_batches/scaling_factor)*Lxx_tilde + regcoef_x/nodes\n",
        "    Lyy_tilde = 0.25*number_samples*radius_x**2\n",
        "    Lyy =  (num_batches/scaling_factor)*Lyy_tilde + regcoef_y/nodes\n",
        "    feature_norm_sum = sum(LA.norm(batch_features, axis=1))\n",
        "    Lxy_tilde = (1+ 0.25*radius_x*radius_y)*number_samples + 0.25*radius_x*feature_norm_sum\n",
        "    Lxy = (num_batches/scaling_factor)*Lxy_tilde\n",
        "    Lyx = Lxy\n",
        "\n",
        "    return Lxx, Lyy, Lxy, Lyx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing global Lipschitz parameters $L_{xx}, L_{yy}, L_{xy}$ and $L_{yx}$"
      ],
      "metadata": {
        "id": "n55zKiw11QXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1qhUIQJtIGs"
      },
      "outputs": [],
      "source": [
        "def global_lipschitz(mini_batch_features,mini_batch_values,radius_x,radius_y,nodes\n",
        "                    ,num_batches,regcoef_x,regcoef_y,scaling_factor):\n",
        "    Lxx_batch = np.zeros(num_batches)\n",
        "    Lyy_batch = np.zeros(num_batches)\n",
        "    Lxy_batch = np.zeros(num_batches)\n",
        "    Lyx_batch = np.zeros(num_batches)\n",
        "    Lxx_nodes = np.zeros(nodes)\n",
        "    Lyy_nodes = np.zeros(nodes)\n",
        "    Lxy_nodes = np.zeros(nodes)\n",
        "    Lyx_nodes = np.zeros(nodes)\n",
        "    for i in range(nodes):\n",
        "        for j in range(num_batches):\n",
        "            Lxx_batch[j],Lyy_batch[j],Lxy_batch[j],Lyx_batch[j] = local_lipschitz_constants(\n",
        "                                            mini_batch_features[i][j],mini_batch_values[i][j]\n",
        "                                            ,radius_x, radius_y,regcoef_x,regcoef_y,\n",
        "                                                scaling_factor)\n",
        "\n",
        "        Lxx_nodes[i] = np.max(Lxx_batch)\n",
        "        Lyy_nodes[i] = np.max(Lyy_batch)\n",
        "        Lxy_nodes[i] = np.max(Lxy_batch)\n",
        "        Lyx_nodes[i] = np.max(Lyx_batch)\n",
        "\n",
        "    return np.max(Lxx_nodes), np.max(Lyy_nodes),np.max(Lxy_nodes), np.max(Lyx_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing gradient of $f_i(x,y)$ with respect to $x$ and $y$."
      ],
      "metadata": {
        "id": "WZKR-BgC8OmE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIsYd1J0tIGs"
      },
      "outputs": [],
      "source": [
        "## This function returns the gradient of f_i(x,y) with respect to x and y.\n",
        "\n",
        "def full_batch_grad(A,b,regcoef_x,regcoef_y,nodes,num_batches,x,y,scaling_factor):\n",
        "    grad_x = np.zeros(A.shape[1])\n",
        "    grad_y = np.zeros(A.shape[1])\n",
        "    local_samples = A.shape[0]\n",
        "    for i in range(local_samples):\n",
        "        perturbed_feature = np.add(A[i],y) ## a_i + y\n",
        "        product = np.dot(x,perturbed_feature) ## xT(a_i +y)\n",
        "        domain = b[i]*product ## bi*xT(a_i +y)\n",
        "        if (domain > 0): ## To avoid math flow error\n",
        "            rec_exp = 1 - 1/(1 + math.exp((-1)*domain)) ## 1/(1+exp^(bi*xT(a_i +y)))\n",
        "\n",
        "        else:\n",
        "            rec_exp = 1/(1 + math.exp(domain))\n",
        "\n",
        "\n",
        "        scalar = (-1)*b[i]*rec_exp\n",
        "        ratio_x = scalar*perturbed_feature\n",
        "        grad_x = np.add(grad_x,ratio_x)\n",
        "\n",
        "        \"gradient w.r.t y\"\n",
        "        ratio_y = scalar*x\n",
        "        grad_y = np.add(grad_y,ratio_y)\n",
        "\n",
        "\n",
        "    loss_der_x = (1/scaling_factor)*grad_x\n",
        "\n",
        "    gradient_x = np.add(loss_der_x,(regcoef_x/nodes)*x)\n",
        "\n",
        "    loss_der_y = (1/scaling_factor)*grad_y\n",
        "\n",
        "    gradient_y = np.subtract(loss_der_y,(regcoef_y/nodes)*y)\n",
        "\n",
        "\n",
        "    return gradient_x , gradient_y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing gradient of $f_{ij}(x,y)$ with respect to $x$ and $y$"
      ],
      "metadata": {
        "id": "DEoYjUVJ8cjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAggXIPwtIGt"
      },
      "outputs": [],
      "source": [
        "## This function returns the gradient of f_{ij}(xy) with respect to x and y.\n",
        "\n",
        "def mini_batch_grad(A,b,regcoef_x,regcoef_y,nodes,x,y,scaling_factor):\n",
        "\n",
        "    \"A = mini-batch features\"\n",
        "    \"b = mini-batch labels\"\n",
        "    batch_size = A.shape[0]\n",
        "    grad_x = np.zeros(A.shape[1])\n",
        "    grad_y = np.zeros(A.shape[1])\n",
        "    for i in range(batch_size):\n",
        "        perturbed_feature = np.add(A[i],y) ## a_i + y\n",
        "        product = np.dot(x,perturbed_feature) ## xT(a_i +y)\n",
        "        domain = b[i]*product ## bi*xT(a_i +y)\n",
        "        if (domain > 0): ## To avoid math flow error\n",
        "            rec_exp = 1 - 1/(1 + math.exp((-1)*domain)) ## 1/(1+exp^(bi*xT(a_i +y)))\n",
        "            #print (num1)\n",
        "        else:\n",
        "            rec_exp = 1/(1 + math.exp(domain))\n",
        "\n",
        "        scalar = (-1)*b[i]*rec_exp\n",
        "        ratio_x = scalar*perturbed_feature\n",
        "        grad_x = np.add(grad_x,ratio_x)\n",
        "\n",
        "        \"gradient w.r.t y\"\n",
        "        ratio_y = scalar*x\n",
        "        grad_y = np.add(grad_y,ratio_y)\n",
        "\n",
        "    loss_der_x = (num_batches/scaling_factor)*grad_x\n",
        "    gradient_x = np.add(loss_der_x,(regcoef_x/nodes)*x)\n",
        "\n",
        "    loss_der_y = (num_batches/scaling_factor)*grad_y\n",
        "    gradient_y = np.subtract(loss_der_y,(regcoef_y/nodes)*y)\n",
        "\n",
        "\n",
        "    return gradient_x, gradient_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T-tyNTYtIGt"
      },
      "outputs": [],
      "source": [
        "## Defining SGD oracle\n",
        "def SGD_oracle(batches_features,batches_values,sample_prob ,nodes,num_batches,x,y,\n",
        "               regcoef_x,regcoef_y,scaling_factor):\n",
        "\n",
        "    \"sample_prob = probability of sampling a mini-batch\"\n",
        "    \"batches_features , batches_values = batches of a particular node\"\n",
        "    indices = np.arange(num_batches)\n",
        "    sampled_batch_list = np.random.choice(indices,1,p = sample_prob) ## sampling a minibatch using sample_prob distribution\n",
        "    sampled_batch = sampled_batch_list[0]\n",
        "\n",
        "    mini_batch_f = batches_features[sampled_batch]\n",
        "    mini_batch_val = batches_values[sampled_batch]\n",
        "\n",
        "    stoch_grad_x, stoch_grad_y = mini_batch_grad(mini_batch_f,mini_batch_val,\n",
        "                                     regcoef_x,regcoef_y,nodes,x,y,scaling_factor)\n",
        "\n",
        "    return stoch_grad_x, stoch_grad_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tNwTF9ptIGt"
      },
      "outputs": [],
      "source": [
        "\"This function returns the stochastic gradient estimator and latest reference point for a particular node\"\n",
        "\n",
        "def svrg_oracle(features,values,batches_features,batches_values,sample_prob\n",
        "                   ,omega,num_batches,ref_point_x,ref_point_y,FB_grad_x, FB_grad_y,x,y\n",
        "                    ,scaling_factor):\n",
        "\n",
        "    \"sample_prob = probability of sampling a mini-batch\"\n",
        "    \"ref_prob = probability with which refrence point is updated\"\n",
        "    \"features, values = full batch features and values of a given node\"\n",
        "    \"batches_features , batches_values = batches of a particular node\"\n",
        "    indices = np.arange(num_batches)\n",
        "    sampled_batch_list = np.random.choice(indices,1,p = sample_prob) ## sampling a minibatch using sample_prob distribution\n",
        "    sampled_batch = sampled_batch_list[0]\n",
        "#     print ('sampled_batch',sampled_batch)\n",
        "    mini_batch_f = batches_features[sampled_batch]\n",
        "    mini_batch_val = batches_values[sampled_batch]\n",
        "    ## computes gradient of fij at (x,y)\n",
        "    grad_x, grad_y = mini_batch_grad(mini_batch_f,mini_batch_val,\n",
        "                                     regcoef_x,regcoef_y,nodes,x,y,scaling_factor)\n",
        "\n",
        "    ## computes gradient at (ref_point_x,ref_point_y)\n",
        "    grad_ref_x, grad_ref_y = mini_batch_grad(mini_batch_f,mini_batch_val,regcoef_x,\n",
        "                                             regcoef_y,nodes,ref_point_x,ref_point_y,scaling_factor)\n",
        "\n",
        "    # grad_x f_il(x,y) - grad_x f_il(ref_x,ref_y)\n",
        "    grad_x_diff = np.subtract(grad_x, grad_ref_x)\n",
        "    ## computing 1/np_il * (grad_x f_il(x,y) - grad_x f_il(ref_x,ref_y))\n",
        "    scale_grad_x_diff = 1/(num_batches*sample_prob[sampled_batch])*grad_x_diff\n",
        "    stoch_grad_x = np.add(scale_grad_x_diff, FB_grad_x)\n",
        "\n",
        "    ## computing stochastic gradient with respect to y\n",
        "\n",
        "    # grad_y f_il(x,y) - grad_y f_il(ref_x,ref_y)\n",
        "    grad_y_diff = np.subtract(grad_y, grad_ref_y)\n",
        "    ## computing 1/np_il * (grad_y f_il(x,y) - grad_y f_il(ref_x,ref_y))\n",
        "    scale_grad_y_diff = 1/(num_batches*sample_prob[sampled_batch])*grad_y_diff\n",
        "    stoch_grad_y = np.add(scale_grad_y_diff, FB_grad_y)\n",
        "\n",
        "#     \"updating reference point\"\n",
        "    if (omega == 1):\n",
        "        ref_point_x = np.copy(x)\n",
        "        ref_point_y = np.copy(y)\n",
        "        ## computes full batch gradient at new (ref_point_x,ref_point_y)\n",
        "        FB_grad_x, FB_grad_y = full_batch_grad(features,values,regcoef_x,\n",
        "                                           regcoef_y,nodes,num_batches,ref_point_x,ref_point_y\n",
        "                                              , scaling_factor)\n",
        "\n",
        "    return stoch_grad_x, stoch_grad_y,FB_grad_x, FB_grad_y, ref_point_x,ref_point_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt_hwvZotIGu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function returns the weighted sum of x_1, x_2, ...,x_m where x = [x_1,...,x_m]\n",
        "\"\"\"\n",
        "\n",
        "def oneConsensus(W,nodes,x):\n",
        "    v = np.zeros((nodes,len(x[0])))\n",
        "    for i in range(nodes):\n",
        "        u = [W[i][j]*np.array(x[j]) for j in range(nodes)]\n",
        "        u = np.array(u)\n",
        "        v[i] = u.sum(axis = 0)\n",
        "\n",
        "    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYobB3WJtIGu"
      },
      "outputs": [],
      "source": [
        "## faster AccGossip. Fast for higher dimensions\n",
        "## x = [x1,x2,....,xm], m-dimensional vector\n",
        "\n",
        "def acce_consensus(W,m,eta,tau,x):\n",
        "\n",
        "    v = np.zeros(nodes)\n",
        "\n",
        "    x_new1 = np.copy(x)\n",
        "    x_old1 = np.copy(x)\n",
        "\n",
        "    for t in range(int(tau)):\n",
        "        x_old2 = np.copy(x_old1)   ## z_k,t-1\n",
        "        x_old1 = np.copy(x_new1)   ## z_k,t\n",
        "        for i in range(m):\n",
        "            v[i] = np.dot(W[i],x_old1)\n",
        "        first_term = (1+eta)*v\n",
        "        sec_term = eta*x_old2\n",
        "\n",
        "        x_new1 = np.subtract(first_term, sec_term)  ## z_{k,t+1}\n",
        "    return x_new1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mVuv3botIGu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Projection of point v onto l2 ball = (radius*v)/max(radius,||v||)\n",
        "\"\"\"\n",
        "\n",
        "def projection_L2ball(v,radius):\n",
        "    norm = LA.norm(v)\n",
        "    if (norm <= radius):\n",
        "        return v\n",
        "    else:\n",
        "        scaling = radius/norm\n",
        "        projection = scaling*v\n",
        "        return projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0G1FEBMtIGu"
      },
      "outputs": [],
      "source": [
        "\"This function returns the sum of difference between xt ,xstar and yt, ystar\"\n",
        "def distance_from_saddle(x,y,xstar,ystar,nodes):\n",
        "    dist_xi_xstar = 0\n",
        "    dist_yi_ystar = 0\n",
        "    for i in range(nodes):\n",
        "        diff_xi = np.subtract(x[i],xstar)\n",
        "        dist_xi_xstar += (LA.norm(diff_xi))**2\n",
        "        diff_yi = np.subtract(y[i],ystar)\n",
        "        dist_yi_ystar += (LA.norm(diff_yi))**2\n",
        "    total_distance = dist_xi_xstar +  dist_yi_ystar\n",
        "    return total_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZxMbYaotIGv"
      },
      "outputs": [],
      "source": [
        "def local_distances(x,y,x0,y0):\n",
        "#     distances = np.zeros(nodes)\n",
        "    diff_x = np.subtract(x,x0)\n",
        "    norm_x = LA.norm(diff_x,axis = 1) ## computes norm of each x[i] - x0[i]\n",
        "    square_norm_x = np.square(norm_x) ## takes elementwise square of ||x[i] - x0[i]||\n",
        "\n",
        "    diff_y = np.subtract(y,y0)\n",
        "    norm_y = LA.norm(diff_y, axis = 1) ## computes norm of each y[i] - y0[i]\n",
        "    square_norm_y = np.square(norm_y) ## takes elementwise square of ||y[i] - y0[i]||\n",
        "\n",
        "    distances = np.add(square_norm_x, square_norm_y) ## ith entry: ||x[i] - x0[i]||^2 + ||y[i] - y0[i]||^2\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rre0BUp1tIGv"
      },
      "outputs": [],
      "source": [
        "## compression operator\n",
        "\n",
        "def qsgd_quantize(x, num_bits):\n",
        "    bits = 2**(num_bits - 1)\n",
        "    norm = LA.norm(x, np.inf)\n",
        "    if (norm <= 10**(-15)): ## if x is zero vector\n",
        "        return x\n",
        "    else:\n",
        "        level_float = bits * np.abs(x) / norm\n",
        "#         print ('level_float',level_float)\n",
        "        previous_level = np.floor(level_float)\n",
        "        is_next_level = np.random.rand(*x.shape) < (level_float - previous_level)\n",
        "#         print ('level_float - previous_level',level_float - previous_level)\n",
        "#         print ('is_next_level',is_next_level)\n",
        "        new_level = previous_level + is_next_level\n",
        "#         print ('previous_level',previous_level)\n",
        "#         print ('new_level',new_level)\n",
        "        return np.sign(x) * norm * new_level / bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmQAh_wytIGv"
      },
      "outputs": [],
      "source": [
        "def delta_qsgd(num_bits,dimension):\n",
        "\n",
        "    bit_rep = 2**(num_bits - 1)\n",
        "\n",
        "    ratio = 1 + min(dimension/(bit_rep**2), math.sqrt(dimension)/bit_rep)\n",
        "\n",
        "    return 1 - 1/ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCgriu_UtIGv"
      },
      "outputs": [],
      "source": [
        "# This function compresses x_1,..x_m and y_1,...,y_m.\n",
        "# Then compressed vectors are communicated among nodes to get WQ(x).\n",
        "\n",
        "def COMM(num_bits,nu_x,H_x,Hw_x,alpha,W,nodes):\n",
        "    ## initializing Q_x, Q_y\n",
        "    Q_x = np.copy(nu_x)\n",
        "    for i in range(nodes):\n",
        "        diff_x = np.subtract(nu_x[i],H_x[i])\n",
        "        Q_x[i]= qsgd_quantize(diff_x, num_bits)\n",
        "\n",
        "    nu_hat_x = np.add(H_x,Q_x)\n",
        "\n",
        "    new_H_x = np.add((1-alpha)*H_x, alpha*nu_hat_x)\n",
        "\n",
        "    WQ_x = oneConsensus(W,nodes,Q_x)\n",
        "\n",
        "    nuW_hat_x = np.add(Hw_x,WQ_x)\n",
        "\n",
        "    new_Hw_x = np.add((1-alpha)*Hw_x,alpha*nuW_hat_x)\n",
        "\n",
        "    return nu_hat_x, nuW_hat_x, new_H_x, new_Hw_x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IPDHG algorithm using GSG oracle"
      ],
      "metadata": {
        "id": "FyFN9pQX8xll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M84kyYrtIGv"
      },
      "outputs": [],
      "source": [
        "def compressed_SGD(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,Kmax): ## Kmax = maximumm number of iterations in IPDHG with GSG oracle\n",
        "\n",
        "    \"Initializations\"\n",
        "    x_t = np.copy(x0) ## current iterate x_t\n",
        "    y_t = np.copy(y0) ## current iterate y_t\n",
        "    xt_hat = np.copy(x0)\n",
        "    yt_hat = np.copy(y0)\n",
        "    nu_xt = np.copy(x0)\n",
        "    nu_yt = np.copy(y0)\n",
        "    Dx_t = np.copy(D_x)\n",
        "    Dy_t = np.copy(D_y)\n",
        "    Hx_t = np.copy(H_x)\n",
        "    Hy_t = np.copy(H_y)\n",
        "    Hw_xt = np.copy(Hw_x)\n",
        "    Hw_yt = np.copy(Hw_y)\n",
        "\n",
        "    dist_from_saddle = np.zeros(Kmax+1)\n",
        "    func_avg = np.zeros(Kmax+1)\n",
        "    consensus_error_x = np.zeros(Kmax)\n",
        "    consensus_error_y = np.zeros(Kmax)\n",
        "    compression_error_x = np.zeros(Kmax)\n",
        "    compression_error_y = np.zeros(Kmax)\n",
        "\n",
        "    \"step size s and other constants alpha, gamma setup\"\n",
        "\n",
        "\n",
        "    bx, by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize)\n",
        "\n",
        "    alpha_x, alpha_y, gamma_x, gamma_y, Mx,My = sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,Lxx,Lyy,\n",
        "                                                            Lxy,Lyx,delta,nodes,lambda_max)\n",
        "\n",
        "    for t in range(Kmax):\n",
        "        for i in range(nodes):\n",
        "            scaled_stoch_grad_x, scaled_stoch_grad_y = SGD_oracle(mini_batch_features[i],mini_batch_values[i],\n",
        "                                                        sample_prob,nodes,num_batches,x_t[i],y_t[i],\n",
        "                                                            regcoef_x,regcoef_y, scaling_factor)\n",
        "\n",
        "            direction_x = np.add(scaled_stoch_grad_x, Dx_t[i])\n",
        "            nu_xt[i] = np.subtract(x_t[i],sgd_stepsize*direction_x)\n",
        "\n",
        "            ## finding nu_yt\n",
        "            direction_y = np.subtract(scaled_stoch_grad_y, Dy_t[i])\n",
        "            nu_yt[i] = np.add(y_t[i],sgd_stepsize*direction_y)\n",
        "\n",
        "        \"compressing nu_xt and nu_yt and communicate the resulting compressed vectors\"\n",
        "        nu_hat_xt, nuW_hat_xt, Hx_t,Hw_xt = COMM(num_bits,nu_xt,Hx_t,Hw_xt,alpha_x,W,nodes)\n",
        "        nu_hat_yt, nuW_hat_yt, Hy_t,Hw_yt = COMM(num_bits,nu_yt,Hy_t,Hw_yt,alpha_y,W,nodes)\n",
        "\n",
        "\n",
        "        diff_hat_nux_nuxW = np.subtract(nu_hat_xt,nuW_hat_xt)\n",
        "        diff_hat_nuy_nuyW = np.subtract(nu_hat_yt,nuW_hat_yt)\n",
        "\n",
        "        ## updating Dxt and Dyt\n",
        "\n",
        "        scale_diff_nux_nuxW = (gamma_x/(2*sgd_stepsize))*diff_hat_nux_nuxW\n",
        "        scale_diff_nuy_nuyW = (gamma_y/(2*sgd_stepsize))*diff_hat_nuy_nuyW\n",
        "\n",
        "        Dx_t = np.add(Dx_t,scale_diff_nux_nuxW)\n",
        "        Dy_t = np.add(Dy_t,scale_diff_nuy_nuyW)\n",
        "\n",
        "        ## upating xt_hat and yt_hat\n",
        "\n",
        "        gamma_diff_nux_nuxW = (gamma_x/2)*diff_hat_nux_nuxW\n",
        "        gamma_diff_nuy_nuyW = (gamma_y/2)*diff_hat_nuy_nuyW\n",
        "\n",
        "        xt_hat = np.subtract(nu_xt,gamma_diff_nux_nuxW)\n",
        "        yt_hat = np.subtract(nu_yt,gamma_diff_nuy_nuyW)\n",
        "        for i in range(nodes):\n",
        "            x_t[i] = projection_L2ball(xt_hat[i],radius_x)\n",
        "            y_t[i] = projection_L2ball(yt_hat[i],radius_y)\n",
        "\n",
        "        \"To compute zT0 - zT0-1, copy xT0-1, yT0-1\"\n",
        "\n",
        "        if (t == Kmax-2):\n",
        "            prev_xt = np.copy(x_t)\n",
        "            prev_yt = np.copy(y_t)\n",
        "\n",
        "        \"Saving important quantities\"\n",
        "\n",
        "        x_ti_avg = x_t.mean(axis = 0) ## cumputes mean of local iterates x_t[i] over i\n",
        "\n",
        "        y_ti_avg = y_t.mean(axis = 0) ## cumputes mean of local iterates y_t[i] over i\n",
        "\n",
        "        func_avg[t+1] = logistic_func(A,b,x_ti_avg,y_ti_avg,nodes,num_batches,regcoef_x,regcoef_y\n",
        "                                     , scaling_factor)\n",
        "\n",
        "        function_values.write(str(func_avg[t+1])+'\\n')\n",
        "        function_values.flush()\n",
        "\n",
        "        ## saving distance from saddle point solution\n",
        "        dist_from_saddle[t+1] = distance_from_saddle(x_t,y_t,xstar,ystar,nodes)\n",
        "        total_distance_from_saddle.write(str(dist_from_saddle[t+1])+'\\n')\n",
        "        total_distance_from_saddle.flush()\n",
        "\n",
        "        ## saving consensus error\n",
        "        cons_error_x = 0\n",
        "        cons_error_y = 0\n",
        "        for i in range(nodes):\n",
        "            consensus_x = np.subtract(x_t[i],x_ti_avg)\n",
        "            cons_error_x += (LA.norm(consensus_x))**2\n",
        "            consensus_y = np.subtract(y_t[i],y_ti_avg)\n",
        "            cons_error_y += (LA.norm(consensus_y))**2\n",
        "\n",
        "        consensus_error_x[t] = cons_error_x\n",
        "        consensus_error_y[t] = cons_error_y\n",
        "        sum_consensus_error_x.write(str(consensus_error_x[t])+'\\n')\n",
        "        sum_consensus_error_x.flush()\n",
        "        sum_consensus_error_y.write(str(consensus_error_y[t])+'\\n')\n",
        "        sum_consensus_error_y.flush()\n",
        "\n",
        "         ## saving compression error nuxhat - nux  and nuyhat - nuy\n",
        "\n",
        "        comp_error_x = np.subtract(nu_hat_xt,nu_xt)\n",
        "        compression_error_x[t] = sum([(LA.norm(comp_error_x[i]))**2 for i in range(nodes)])\n",
        "        compression_error_nux.write(str(compression_error_x[t])+'\\n')\n",
        "        compression_error_nux.flush()\n",
        "\n",
        "        comp_error_y = np.subtract(nu_hat_yt,nu_yt)\n",
        "        compression_error_y[t] = sum([(LA.norm(comp_error_y[i]))**2 for i in range(nodes)])\n",
        "        compression_error_nuy.write(str(compression_error_y[t])+'\\n')\n",
        "        compression_error_nuy.flush()\n",
        "\n",
        "        \"Saving completed!\"\n",
        "\n",
        "    dist_local_zt_z0 = local_distances(x_t,y_t,prev_xt,prev_yt)\n",
        "    avg_sgd_dist = acce_consensus(W,nodes,eta,tau,dist_local_zt_z0)\n",
        "    full_sgd_distances = {'full_sgd_loc_dist':dist_local_zt_z0,'full_sgd_avg_dist':avg_sgd_dist}\n",
        "\n",
        "    return x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt,Hw_yt,full_sgd_distances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IPDHG with SVRG oracle"
      ],
      "metadata": {
        "id": "YQ9EJ4QV885E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpTWNN7TtIGw"
      },
      "outputs": [],
      "source": [
        "def compressed_SVRG(A,b,x0,y0,Dx,Dy,Hx,Hy,Hwx,Hwy,svrg_stepsize,T):\n",
        "    \"features, values = local samples of all nodes\"\n",
        "    \"mini_batch_features, mini_batch_values = mini-batches of all nodes \"\n",
        "    dist_from_saddle = np.zeros(T+1)\n",
        "    func_avg = np.zeros(T+1)\n",
        "    omega = np.zeros(T) ## keeps track of full batch gradient computations\n",
        "    consensus_error_x = np.zeros(T)\n",
        "    consensus_error_y = np.zeros(T)\n",
        "\n",
        "    compression_error_x = np.zeros(T)\n",
        "    compression_error_y = np.zeros(T)\n",
        "    x_t = np.copy(x0) ## current iterate x_t\n",
        "    y_t = np.copy(y0) ## current iterate y_t\n",
        "    xt_hat = np.copy(x0)\n",
        "    yt_hat = np.copy(y0)\n",
        "    nu_xt = np.copy(x0)\n",
        "    nu_yt = np.copy(y0)\n",
        "    Dx_t = np.copy(Dx)\n",
        "    Dy_t = np.copy(Dy)\n",
        "    Hx_t = np.copy(Hx)\n",
        "    Hy_t = np.copy(Hy)\n",
        "    Hw_xt = np.copy(Hwx)\n",
        "    Hw_yt = np.copy(Hwy)\n",
        "\n",
        "\n",
        "    \"reference points are initialized to the last primal and dual iterates of IPDHG + SGD oracle\"\n",
        "    ref_point_x = np.copy(x_t)\n",
        "    ref_point_y = np.copy(y_t)\n",
        "\n",
        "    ## Initial full batch gradient\n",
        "    FB_grad_x = [[] for i in range(nodes)]\n",
        "    FB_grad_y = [[] for i in range(nodes)]\n",
        "    for i in range(nodes):\n",
        "        FB_grad_x[i],FB_grad_y[i] = full_batch_grad(features[i],values[i],regcoef_x,regcoef_y,nodes,num_batches,\n",
        "                                                     ref_point_x[i],ref_point_y[i],scaling_factor)\n",
        "\n",
        "    \"parameters setup for svrg\"\n",
        "\n",
        "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
        "\n",
        "\n",
        "    alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(\n",
        "                                                    mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W)\n",
        "\n",
        "    Mx, My = MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta)\n",
        "\n",
        "    for t in range(T):\n",
        "        ## generate a Bernoulli rv with prob ref_prob\n",
        "        omega[t] = bernoulli.rvs(ref_prob)\n",
        "        ## saving omega values\n",
        "        full_batch_grad_counts.write(str(omega[t])+'\\n')\n",
        "        full_batch_grad_counts.flush()\n",
        "        ## computing gradient steps\n",
        "        for i in range(nodes):\n",
        "            ## compute stochastic gradient estimator using svrg oracle\n",
        "            svrg_grad_x,svrg_grad_y,FB_grad_x[i],FB_grad_y[i],ref_point_x[i],ref_point_y[i] = svrg_oracle(\n",
        "                                                            features[i],values[i],mini_batch_features[i],\n",
        "                                                          mini_batch_values[i],sample_prob, omega[t],num_batches\n",
        "                                                                 ,ref_point_x[i], ref_point_y[i],FB_grad_x[i],\n",
        "                                                                   FB_grad_y[i],x_t[i],y_t[i],scaling_factor)\n",
        "\n",
        "\n",
        "            direction_x = np.add(svrg_grad_x, Dx_t[i])\n",
        "            nu_xt[i] = np.subtract(x_t[i],svrg_stepsize*direction_x)\n",
        "\n",
        "            ## finding nu_yt\n",
        "            direction_y = np.subtract(svrg_grad_y, Dy_t[i])\n",
        "            nu_yt[i] = np.add(y_t[i],svrg_stepsize*direction_y)\n",
        "\n",
        "        \"compressing nu_xt and nu_yt and communicate the resulting compressed vectors\"\n",
        "        nu_hat_xt, nuW_hat_xt, Hx_t,Hw_xt = COMM(num_bits,nu_xt,Hx_t,Hw_xt,alpha_x,W,nodes)\n",
        "        nu_hat_yt, nuW_hat_yt, Hy_t,Hw_yt = COMM(num_bits,nu_yt,Hy_t,Hw_yt,alpha_y,W,nodes)\n",
        "\n",
        "\n",
        "        diff_hat_nux_nuxW = np.subtract(nu_hat_xt,nuW_hat_xt)\n",
        "        diff_hat_nuy_nuyW = np.subtract(nu_hat_yt,nuW_hat_yt)\n",
        "\n",
        "        ## updating Dxt and Dyt\n",
        "\n",
        "        scale_diff_nux_nuxW = (gamma_x/(2*svrg_stepsize))*diff_hat_nux_nuxW\n",
        "        scale_diff_nuy_nuyW = (gamma_y/(2*svrg_stepsize))*diff_hat_nuy_nuyW\n",
        "\n",
        "        Dx_t = np.add(Dx_t,scale_diff_nux_nuxW)\n",
        "        Dy_t = np.add(Dy_t,scale_diff_nuy_nuyW)\n",
        "\n",
        "        ## upating xt_hat and yt_hat\n",
        "\n",
        "        gamma_diff_nux_nuxW = (gamma_x/2)*diff_hat_nux_nuxW\n",
        "        gamma_diff_nuy_nuyW = (gamma_y/2)*diff_hat_nuy_nuyW\n",
        "\n",
        "        xt_hat = np.subtract(nu_xt,gamma_diff_nux_nuxW)\n",
        "        yt_hat = np.subtract(nu_yt,gamma_diff_nuy_nuyW)\n",
        "        for i in range(nodes):\n",
        "            x_t[i] = projection_L2ball(xt_hat[i],radius_x)\n",
        "            y_t[i] = projection_L2ball(yt_hat[i],radius_y)\n",
        "\n",
        "        \"Saving required quantities\"\n",
        "\n",
        "        x_ti_avg = x_t.mean(axis = 0) ## cumputes mean of local iterates x_t[i] over i\n",
        "\n",
        "        y_ti_avg = y_t.mean(axis = 0) ## cumputes mean of local iterates y_t[i] over i\n",
        "\n",
        "        func_avg[t+1] = logistic_func(A,b,x_ti_avg,y_ti_avg,nodes,num_batches,regcoef_x,regcoef_y\n",
        "                                     , scaling_factor)\n",
        "\n",
        "        function_values.write(str(func_avg[t+1])+'\\n')\n",
        "        function_values.flush()\n",
        "\n",
        "        ## saving distance from saddle point solution\n",
        "        dist_from_saddle[t+1] = distance_from_saddle(x_t,y_t,xstar,ystar,nodes)\n",
        "        total_distance_from_saddle.write(str(dist_from_saddle[t+1])+'\\n')\n",
        "        total_distance_from_saddle.flush()\n",
        "\n",
        "        ## saving consensus error\n",
        "        cons_error_x = 0\n",
        "        cons_error_y = 0\n",
        "        for i in range(nodes):\n",
        "            consensus_x = np.subtract(x_t[i],x_ti_avg)\n",
        "            cons_error_x += (LA.norm(consensus_x))**2\n",
        "            consensus_y = np.subtract(y_t[i],y_ti_avg)\n",
        "            cons_error_y += (LA.norm(consensus_y))**2\n",
        "\n",
        "        consensus_error_x[t] = cons_error_x\n",
        "        consensus_error_y[t] = cons_error_y\n",
        "        sum_consensus_error_x.write(str(consensus_error_x[t])+'\\n')\n",
        "        sum_consensus_error_x.flush()\n",
        "        sum_consensus_error_y.write(str(consensus_error_y[t])+'\\n')\n",
        "        sum_consensus_error_y.flush()\n",
        "\n",
        "         ## saving compression error nuxhat - nux  and nuyhat - nuy\n",
        "\n",
        "        comp_error_x = np.subtract(nu_hat_xt,nu_xt)\n",
        "        compression_error_x[t] = sum([(LA.norm(comp_error_x[i]))**2 for i in range(nodes)])\n",
        "        compression_error_nux.write(str(compression_error_x[t])+'\\n')\n",
        "        compression_error_nux.flush()\n",
        "\n",
        "        comp_error_y = np.subtract(nu_hat_yt,nu_yt)\n",
        "        compression_error_y[t] = sum([(LA.norm(comp_error_y[i]))**2 for i in range(nodes)])\n",
        "        compression_error_nuy.write(str(compression_error_y[t])+'\\n')\n",
        "        compression_error_nuy.flush()\n",
        "\n",
        "        \"Saving completed!\"\n",
        "\n",
        "    return x_t,y_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical approach to switch from GSGO to SVRGO"
      ],
      "metadata": {
        "id": "_1IYoj-q9frN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5xHwotTtIGw"
      },
      "outputs": [],
      "source": [
        "def heuristic_switch(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,svrg_stepsize,initial_Kmax,\n",
        "                      sgd_epsilon,Tsvrg,threshold):\n",
        "\n",
        "    \"T0 = number of iterations using SG oracle to approximate Phi_0 \"\n",
        "    \"We replace xstar, ystar in Phi0 by xT0, yT0\"\n",
        "\n",
        "    old_Kmax = initial_Kmax ## setting up T0\n",
        "\n",
        "\n",
        "    \"calling IPDHG with SG oracle for T0 = initial_Kmax iterations\"\n",
        "\n",
        "    x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt,Hw_yt,full_sgd_distances = compressed_SGD(A,b,x0,y0,D_x,D_y,H_x,H_y,\n",
        "                                                                       Hw_x,Hw_y,sgd_stepsize,\n",
        "                                                                           initial_Kmax)\n",
        "\n",
        "    \"accessing local distances between last two consecutive iterations and their average\"\n",
        "\n",
        "    full_sgd_dist_local_zt_z0 = full_sgd_distances['full_sgd_loc_dist']\n",
        "    avg_full_sgd_dist = full_sgd_distances['full_sgd_avg_dist']\n",
        "\n",
        "\n",
        "    print ('avg_full_sgd_dist',avg_full_sgd_dist)\n",
        "\n",
        "    \"Continue with SG oracle if difference between last two iterates xT0, xT0-1 is not very small\"\n",
        "\n",
        "    print ('Gap between two last iterates is sufficient?',np.any(avg_full_sgd_dist > threshold))\n",
        "\n",
        "#     print (np.any(avg_full_sgd_dist > threshold) == True)\n",
        "    if (np.any(avg_full_sgd_dist > threshold) == True): ## 1 means all entries in avg_sgd_dist are less than equal to threshold\n",
        "\n",
        "        new_epsilon = best_epsilon0(target_acc,delta,initial_x0,initial_y0,x_t,y_t,sgd_stepsize,\n",
        "                                        features,values,W)\n",
        "\n",
        "        print ('new epsilon0',new_epsilon)\n",
        "\n",
        "        \"Computing T_{j+1}\"\n",
        "\n",
        "        new_Kmax = sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,new_epsilon)\n",
        "\n",
        "        print ('new Kmax',new_Kmax)\n",
        "        print ('old Kmax',old_Kmax)\n",
        "\n",
        "        \"Computing T{j+1}- T_{j} \"\n",
        "        Kmax = new_Kmax - old_Kmax ## T{j+1}- T_{j}\n",
        "        print ('Iterations Tj+1,Tj,Tj+1 - Tj', [new_Kmax,old_Kmax,Kmax])\n",
        "\n",
        "        old_Kmax = new_Kmax\n",
        "#         Kmax = 100 ## to check subsequent steps\n",
        "\n",
        "        \"Perform Kmax = T{j+1}- T_{j} iterations using SG oracle\"\n",
        "        x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t, Hw_xt, Hw_yt,sgd_distances = compressed_SGD(A,b,\n",
        "                                                                                x_t,y_t,Dx_t,Dy_t,\n",
        "                                                                                Hx_t,Hy_t,Hw_xt,\n",
        "                                                                                Hw_yt,sgd_stepsize,Kmax)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"switch to IPDHG with svrg oracle\"\n",
        "\n",
        "    print ('Switch to SVRG oracle')\n",
        "    x_t, y_t = compressed_SVRG(A,b,x_t, y_t, Dx_t, Dy_t, Hx_t,Hy_t,Hw_xt, Hw_yt,svrg_stepsize,Tsvrg)\n",
        "\n",
        "\n",
        "    return x_t,y_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apDYsSpWtIGx"
      },
      "outputs": [],
      "source": [
        "def computing_phi0(Mx,My,gamma_x,gamma_y,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W):\n",
        "\n",
        "    \"Mx, My, gamma_x, gamma_y, alpha_x, alpha_y are the parameters associated with sgd oracle\"\n",
        "    \"zT0 = (xT0,yT0) are the last iterates of IPDHG with SG oracle\"\n",
        "\n",
        "    I = np.identity(nodes)\n",
        "    I_W = np.subtract(I,W)\n",
        "\n",
        "#     local_phi0 = np.zeros(nodes)\n",
        "\n",
        "    \"store full batch gradients at f_i(zT0)\"\n",
        "\n",
        "    FB_grad_x = np.zeros((nodes,dimension_x))\n",
        "    FB_grad_y = np.zeros((nodes,dimension_y))\n",
        "\n",
        "    \"stores difference of grad f_i and average (over nodes) gradients f_i to obtain (i-J)grad F \"\n",
        "\n",
        "    grad_diff_x = np.zeros((nodes,dimension_x))\n",
        "    grad_diff_y = np.zeros((nodes,dimension_y))\n",
        "\n",
        "\n",
        "    \"computing ||x0-xT0||^2\"\n",
        "\n",
        "    diff_x0_xT0 = np.subtract(x0,xT0)\n",
        "    norm_x0_xT0 = (LA.norm(diff_x0_xT0))**2\n",
        "#     Mx_xstar = Mx*nodes*norm_x0_xstar\n",
        "\n",
        "    \"computing ||y0-yT0||^2\"\n",
        "\n",
        "    diff_y0_yT0 = np.subtract(y0,yT0)\n",
        "    norm_y0_yT0 = (LA.norm(diff_y0_yT0))**2\n",
        "#     My_ystar = My*nodes*norm_y0_ystar\n",
        "\n",
        "    for i in range(nodes):\n",
        "        \"computing local full batch gradients\"\n",
        "        FB_grad_x[i], FB_grad_y[i] = full_batch_grad(features[i],values[i],regcoef_x,regcoef_y,nodes,num_batches,\n",
        "                                                     xT0[i],yT0[i],scaling_factor)\n",
        "\n",
        "    avg_grad_x = FB_grad_x.mean(axis = 0) ## average of grad_x f_i's\n",
        "    avg_grad_y = FB_grad_y.mean(axis = 0) ## average of grad_y f_i 's'\n",
        "\n",
        "    for i in range(nodes):\n",
        "        grad_diff_x[i] = np.subtract(FB_grad_x[i],avg_grad_x)\n",
        "        grad_diff_y[i] = np.subtract(FB_grad_y[i],avg_grad_y)\n",
        "\n",
        "    pseudo_inv_I_W = np.linalg.pinv(I_W) ## pseudoinverse of I-W\n",
        "    sqrt_pinv_I_W = scipy.linalg.sqrtm(pseudo_inv_I_W, disp=True, blocksize=64) ## square root of I-W pseudoinverse\n",
        "\n",
        "    pinv_grad_diff_x = np.matmul(sqrt_pinv_I_W,grad_diff_x)\n",
        "    pinv_grad_diff_y = np.matmul(sqrt_pinv_I_W,grad_diff_y)\n",
        "\n",
        "    norm_pinv_grad_diff_x = (LA.norm(pinv_grad_diff_x))**2 ## ||(I-J)grad_x F(1zstar)||^2_{pinv_I-W}\n",
        "    norm_pinv_grad_diff_y = (LA.norm(pinv_grad_diff_y))**2 ## ||(I-J)grad_y F(1zstar)||^2_{pinv_I-W}\n",
        "\n",
        "    \"computing square norm of H0x - Hstar_x and H0y - Hstar_y\"\n",
        "\n",
        "    tile_avg_grad_x = np.tile(avg_grad_x,(nodes,1))\n",
        "    tile_avg_grad_y = np.tile(avg_grad_y,(nodes,1))\n",
        "\n",
        "    diff_H0x_Hstar_x =  np.add(diff_x0_xT0, sgd_stepsize*tile_avg_grad_x)\n",
        "    diff_H0y_Hstar_y =  np.subtract(diff_y0_yT0, sgd_stepsize*tile_avg_grad_y)\n",
        "\n",
        "\n",
        "    norm_diff_H0x_Hstar_x = (LA.norm(diff_H0x_Hstar_x))**2\n",
        "    norm_diff_H0y_Hstar_y = (LA.norm(diff_H0y_Hstar_y))**2\n",
        "\n",
        "    phi_0 = Mx*norm_x0_xT0 + My*norm_y0_yT0 + (2*sgd_stepsize**2)/gamma_x*norm_pinv_grad_diff_x + (2*sgd_stepsize**2)/gamma_y*norm_pinv_grad_diff_y + math.sqrt(delta)*(norm_diff_H0x_Hstar_x + norm_diff_H0y_Hstar_y)\n",
        "\n",
        "\n",
        "    return phi_0,(LA.norm(avg_grad_x))**2, (LA.norm(avg_grad_y))**2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3qNIwkatIGx"
      },
      "outputs": [],
      "source": [
        "\"best epsilon0 using approximation of phi0\"\n",
        "\n",
        "def best_epsilon0(target_acc,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W):\n",
        "\n",
        "    \"parameters for svrg\"\n",
        "\n",
        "    tilde_cx, tilde_cy, svrg_bx, svrg_by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
        "\n",
        "\n",
        "    svrg_alpha_x, svrg_alpha_y, svrg_gamma_x, svrg_gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(\n",
        "                                                            mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W)\n",
        "\n",
        "    svrg_Mx, svrg_My = MxMy(svrg_alpha_x, svrg_alpha_y, svrg_gamma_x, svrg_gamma_y,lambda_max,delta)\n",
        "\n",
        "    \"Computes Phi0\"\n",
        "\n",
        "\n",
        "    sgd_bx, sgd_by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize) ## sgd param\n",
        "    sgd_alpha_x, sgd_alpha_y, sgd_gamma_x, sgd_gamma_y, sgd_Mx, sgd_My = sgd_parameters_alpha_gammaMxMy(sgd_bx,sgd_by,mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max)\n",
        "\n",
        "#     computing_phi0(Mx,My,gamma_x,gamma_y,delta,x0,y0,xT0,yT0,sgd_stepsize,features,values,W)\n",
        "    phi_0,norm_avg_grad_x,norm_avg_grad_y = computing_phi0(sgd_Mx,sgd_My,sgd_gamma_x,sgd_gamma_y,delta,x0,y0,\n",
        "                                                 xT0,yT0,sgd_stepsize,features,values,W)\n",
        "\n",
        "    print ('phi0',phi_0)\n",
        "    svrg_rho = rho_svrg(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize,W)\n",
        "\n",
        "    \"Computes Cmax\"\n",
        "\n",
        "    first_term  = (svrg_Mx+tilde_cx)/sgd_Mx\n",
        "    sec_term = (svrg_My+tilde_cy)/sgd_My\n",
        "    third_term = (sgd_gamma_x*svrg_stepsize**2)/(svrg_gamma_x*sgd_stepsize**2)\n",
        "    fourth_term = (sgd_gamma_y*svrg_stepsize**2)/(svrg_gamma_y*sgd_stepsize**2)\n",
        "\n",
        "\n",
        "    Cmax = max( first_term,sec_term,third_term,fourth_term, 2 )\n",
        "\n",
        "    epsilon0 = target_acc/(2*Cmax*phi_0)\n",
        "\n",
        "\n",
        "    print ('best epsilon0',epsilon0)\n",
        "\n",
        "    return epsilon0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating 2D-Torus topology"
      ],
      "metadata": {
        "id": "F6A80tn290Q3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQUcK9HttIGx"
      },
      "outputs": [],
      "source": [
        "def empty_graph(n=0,create_using=None):\n",
        "    \"\"\"Return the empty graph with n nodes and zero edges.\n",
        "\n",
        "    Node labels are the integers 0 to n-1\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if create_using is None:\n",
        "        # default empty graph is a simple graph\n",
        "        G=nx.Graph()\n",
        "    else:\n",
        "        G=create_using\n",
        "        G.clear()\n",
        "\n",
        "    G.add_nodes_from(range(n))\n",
        "    G.name=\"empty_graph(%d)\"%n\n",
        "    return G\n",
        "\n",
        "\n",
        "def grid_2d(m,n,periodic=False,create_using=None): ## m,n be the number of rows and number of\n",
        "    # columns in torus topolgy\n",
        "\n",
        "    \"\"\" Return the 2d grid graph of mxn nodes,\n",
        "        each connected to its nearest neighbors.\n",
        "        Optional argument periodic=True will connect\n",
        "        boundary nodes via periodic boundary conditions.\n",
        "    \"\"\"\n",
        "\n",
        "    G=empty_graph(0,create_using)\n",
        "    G.name=\"grid_2d_graph\"\n",
        "    rows=range(m)\n",
        "    columns=range(n)\n",
        "    G.add_nodes_from( (i,j) for i in rows for j in columns )\n",
        "    G.add_edges_from( ((i,j),(i-1,j)) for i in rows for j in columns if i>0 )\n",
        "    G.add_edges_from( ((i,j),(i,j-1)) for i in rows for j in columns if j>0 )\n",
        "    if G.is_directed():\n",
        "        G.add_edges_from( ((i,j),(i+1,j)) for i in rows for j in columns if i<m-1 )\n",
        "        G.add_edges_from( ((i,j),(i,j+1)) for i in rows for j in columns if j<n-1 )\n",
        "    if periodic:\n",
        "        if n>2:\n",
        "            G.add_edges_from( ((i,0),(i,n-1)) for i in rows )\n",
        "            if G.is_directed():\n",
        "                G.add_edges_from( ((i,n-1),(i,0)) for i in rows )\n",
        "        if m>2:\n",
        "            G.add_edges_from( ((0,j),(m-1,j)) for j in columns )\n",
        "            if G.is_directed():\n",
        "                G.add_edges_from( ((m-1,j),(0,j)) for j in columns )\n",
        "        G.name=\"periodic_grid_2d_graph(%d,%d)\"%(m,n)\n",
        "    return G\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Computing weight matrix $W$ associated with 2D-Torus topology"
      ],
      "metadata": {
        "id": "m5mHtgyX96bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2rRylCtIGx"
      },
      "outputs": [],
      "source": [
        "def gen_graph(row,column,m):\n",
        "    W = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
        "\n",
        "    \"Generating 2D Grid\"\n",
        "    G = grid_2d(row,column,periodic=False,create_using=None)\n",
        "\n",
        "#     \"Adding extra edges to get 2D Torus\"\n",
        "#     edges_rows = [((0,0),(0,4)),((1,0),(1,4)),((2,0),(2,4)),((3,0),(3,4))]\n",
        "#     column_rows = [((0,0),(3,0)),((0,1),(3,1)),((0,2),(3,2)),((0,3),(3,3)),((0,4),(3,4))]\n",
        "#     G.add_edges_from(edges_rows)\n",
        "#     G.add_edges_from(column_rows)\n",
        "    nx.draw_networkx(G)\n",
        "    plt.savefig('2D-Grid')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    \"Changing edges format to 1D so that it becomes easy to access the indices\"\n",
        "\n",
        "    edges = []\n",
        "    for i in range(row):\n",
        "        for j in range(column-1):\n",
        "            edges.append([j+i*column,j+1+i*column])\n",
        "\n",
        "    for i in range(row-1):\n",
        "        for j in range(column):\n",
        "            edges.append([j+i*column,j+(i+1)*column])\n",
        "\n",
        "    \"Adding extra edges to get 2D Torus from 2D grid\"\n",
        "\n",
        "    for i in range(column):\n",
        "        edges.append([i , i + (row-1)*column])\n",
        "\n",
        "    for i in range(row):\n",
        "        edges.append([i*column , row + i*column])\n",
        "\n",
        "    print ('edges:',edges)\n",
        "    print ('total edges in 2D Torus:',len(edges))\n",
        "\n",
        "    for (u, v) in edges:\n",
        "        W[u][v] = 1/5\n",
        "        W[v][u] = 1/5\n",
        "\n",
        "\n",
        "    for i in range(m):\n",
        "        W[i][i] = 1/5\n",
        "\n",
        "\n",
        "    B = np.matrix(W)\n",
        "    print ((B.transpose() == W).all()) ## check symmetric property of B. It will print True\n",
        "    print ([sum(W[i]) for i in range(m)])\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm8OsABdtIGx"
      },
      "outputs": [],
      "source": [
        "# Computing parameters like step size, tilde_cx, tilde_cy, bx and by for svrg oracle\n",
        "def parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize):\n",
        "    mu = min(mux,muy)\n",
        "    L = max(Lxx,Lyy,Lxy,Lyx)\n",
        "\n",
        "    num_tilde_cx = 8*svrg_stepsize**2*(Lxx**2 + Lyx**2)\n",
        "    tilde_cx = num_tilde_cx/ref_prob ## Taking uniform sample distribution\n",
        "    num_tilde_cy = 8*svrg_stepsize**2*(Lyy**2 + Lxy**2)\n",
        "    tilde_cy = num_tilde_cy/ref_prob\n",
        "    bx = svrg_stepsize*mux - 4*svrg_stepsize**2*Lyx**2 - tilde_cx*ref_prob\n",
        "    by = svrg_stepsize*muy - 4*svrg_stepsize**2*Lxy**2 - tilde_cy*ref_prob\n",
        "\n",
        "    return tilde_cx, tilde_cy, bx, by\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNK_wQrKtIGy"
      },
      "outputs": [],
      "source": [
        "# Computing parameters for svrg oracle\n",
        "\n",
        "def parameters_alpha_gamma(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,delta,nodes,W):\n",
        "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
        "    alpha_x = bx/(1+delta)\n",
        "    alpha_y = by/(1+delta)\n",
        "    I = np.identity(nodes)\n",
        "    I_W = np.subtract(I,W)\n",
        "    eigvalues , eigvectors = eig(I_W)\n",
        "    eigvalues = np.sort(eigvalues) ## sort eigenvalues in increasing order\n",
        "    lambda_max = eigvalues[-1]\n",
        "    lambda_second_small = eigvalues[1]\n",
        "    \"gamma_x\"\n",
        "    gamma_x_second = 1/(4*(1+delta)*lambda_max)\n",
        "    gamma_x_first = gamma_x_second*(bx/math.sqrt(delta))\n",
        "    gamma_x = min(gamma_x_first, gamma_x_second)\n",
        "\n",
        "    \"gamma_y\"\n",
        "    gamma_y_first = gamma_x_second*(by/math.sqrt(delta))\n",
        "    gamma_y = min(gamma_y_first,gamma_x_second)\n",
        "\n",
        "    return alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1mgUvo1tIGy"
      },
      "outputs": [],
      "source": [
        "# Computing parameters for svrg oracle\n",
        "\n",
        "def MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta):\n",
        "    \"Mx and My\"\n",
        "    Mx = 1 - (math.sqrt(delta)*alpha_x)/(1-0.5*gamma_x*lambda_max)\n",
        "    My = 1 - (math.sqrt(delta)*alpha_y)/(1-0.5*gamma_y*lambda_max)\n",
        "    return Mx,My\n",
        "\n",
        "\n",
        "\n",
        "def rho_svrg(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize,W):\n",
        "\n",
        "    tilde_cx, tilde_cy, bx, by = parameters(mux,muy,Lxx,Lyy,Lxy,Lyx,ref_prob,svrg_stepsize)\n",
        "\n",
        "    alpha_x, alpha_y, gamma_x, gamma_y,lambda_second_small,lambda_max = parameters_alpha_gamma(mux,muy,\n",
        "                                                                        Lxx,Lyy,Lxy,Lyx,ref_prob,\n",
        "                                                                            delta,nodes,W)\n",
        "    Mx,My = MxMy(alpha_x, alpha_y, gamma_x, gamma_y,lambda_max,delta)\n",
        "\n",
        "    T1 = (1-bx)/Mx\n",
        "    T2 = (1-by)/My\n",
        "    T3 = 1 - 0.5*gamma_x*lambda_second_small\n",
        "    T4 = 1 - 0.5*gamma_y*lambda_second_small\n",
        "    T5 = 1-alpha_x\n",
        "    T6 = 1-alpha_y\n",
        "    rho = max(T1,T2,T3,T4,T5,T6)\n",
        "\n",
        "    return rho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttIT_ylGtIGy"
      },
      "outputs": [],
      "source": [
        "# Computing parameters for GSG oracle\n",
        "\n",
        "\n",
        "def sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize):\n",
        "\n",
        "    bx = sgd_stepsize*mux - 4*sgd_stepsize**2*Lyx**2\n",
        "    by = sgd_stepsize*muy - 4*sgd_stepsize**2*Lxy**2\n",
        "\n",
        "    return bx, by\n",
        "\n",
        "def sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max):\n",
        "\n",
        "    alpha_x = bx/(1+delta)\n",
        "    alpha_y = by/(1+delta)\n",
        "\n",
        "    \"gamma_x\"\n",
        "    gamma_x_second = 1/(2*((1+delta)**2)*lambda_max)\n",
        "    gamma_x = bx*gamma_x_second\n",
        "\n",
        "    \"gamma_y\"\n",
        "    gamma_y = by*gamma_x_second\n",
        "\n",
        "    \"Mx,My\"\n",
        "\n",
        "    Mx = 1 - (math.sqrt(delta)*alpha_x)/(1-0.5*gamma_x*lambda_max)\n",
        "    My = 1 - (math.sqrt(delta)*alpha_y)/(1-0.5*gamma_y*lambda_max)\n",
        "\n",
        "    return alpha_x, alpha_y, gamma_x, gamma_y, Mx, My\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-IMe2XptIGy"
      },
      "outputs": [],
      "source": [
        "def sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,sgd_epsilon):\n",
        "\n",
        "    bx,by = sgd_param_bx_by(mux,muy,Lxx,Lyy,Lxy,Lyx,sgd_stepsize)\n",
        "    alpha_x, alpha_y, gamma_x, gamma_y, Mx, My = sgd_parameters_alpha_gammaMxMy(bx,by,mux,muy,\n",
        "                                                            Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max)\n",
        "\n",
        "    T1 = (1-bx)/Mx\n",
        "    T2 = (1-by)/My\n",
        "    T3 = 1 - 0.5*gamma_x*lambda_second_small\n",
        "    T4 = 1 - 0.5*gamma_y*lambda_second_small\n",
        "    T5 = 1-alpha_x\n",
        "    T6 = 1-alpha_y\n",
        "    rho = max(T1,T2,T3,T4,T5,T6)\n",
        "    denominator = (-1)*math.log(rho)\n",
        "    T = -1*math.log(sgd_epsilon)/denominator\n",
        "\n",
        "    return int(T)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializations"
      ],
      "metadata": {
        "id": "OyNQdqmv-Ob0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mObDUbXTtIGy"
      },
      "outputs": [],
      "source": [
        "sgd_epsilon = 0.5\n",
        "row = 4 ### total number of rows in 2D Torus\n",
        "column = 5 ### total number of columns in 2D Torus. Note that column = row + 1\n",
        "nodes = row*column ### total number of nodes\n",
        "\n",
        "num_batches = 20 ## This must be fixed for comparison\n",
        "ref_prob = 1/num_batches\n",
        "\n",
        "print('ref prob',ref_prob)\n",
        "\n",
        "regcoef_x = 10\n",
        "regcoef_y = 10\n",
        "\n",
        "mux = regcoef_x\n",
        "muy = regcoef_y\n",
        "\n",
        "radius_x = 100\n",
        "radius_y = 1\n",
        "\n",
        "\n",
        "scaling_factor = A.shape[0]\n",
        "\n",
        "dimension_x = A.shape[1]\n",
        "dimension_y = dimension_x\n",
        "\n",
        "\n",
        "## finding delta\n",
        "\n",
        "num_bits = 4\n",
        "delta = delta_qsgd(num_bits,dimension_x)\n",
        "\n",
        "\n",
        "sample_prob = (1/num_batches)*np.ones(num_batches)\n",
        "print ('sample prob',sample_prob)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayUMs8B9tIGz"
      },
      "outputs": [],
      "source": [
        "## Generating weight matrix W\n",
        "\n",
        "W = gen_graph(row,column,nodes)\n",
        "\n",
        "eigenvalues_W, eigenvectors_W = eig(W)\n",
        "\n",
        "sorted_eigenvalues_W = np.sort(eigenvalues_W)\n",
        "print ('eigenvalues of W', sorted_eigenvalues_W)\n",
        "\n",
        "print ('delta',delta)\n",
        "\n",
        "\n",
        "I = np.identity(nodes)\n",
        "I_W = np.subtract(I,W)\n",
        "eigvalues , eigvectors = eig(I_W)\n",
        "eigvalues = np.sort(eigvalues) ## sort eigenvalues in increasing order\n",
        "lambda_max = eigvalues[-1]\n",
        "lambda_second_small = eigvalues[1]\n",
        "\n",
        "\n",
        "\"constants eta and tau used in accelerated consensus method \"\n",
        "\n",
        "muW = sorted_eigenvalues_W[nodes-2] ## second largest eigen value of W\n",
        "print ('muW',muW)\n",
        "eta = (1 - math.sqrt(1 - muW**2))/(1 + math.sqrt(1 - muW**2))\n",
        "tau = 20 ## iterations in accelerated consensus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwanSbnftIGz"
      },
      "outputs": [],
      "source": [
        "## distribute data point among nodes\n",
        "features, values = data_blocks(A,b,nodes)\n",
        "\n",
        "## creating mnibatches for all nodes\n",
        "\n",
        "mini_batch_features,mini_batch_values = nodes_mini_batches(features,values,nodes,num_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVb0wLUFtIG9"
      },
      "outputs": [],
      "source": [
        "Lxx,Lyy,Lxy,Lyx = global_lipschitz(mini_batch_features,mini_batch_values,radius_x,radius_y,nodes\n",
        "                    ,num_batches,regcoef_x,regcoef_y,scaling_factor)\n",
        "\n",
        "print ('global Lipschitz parameters',(Lxx,Lyy,Lxy,Lyx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utvsmWv1tIG9"
      },
      "outputs": [],
      "source": [
        "\"Initialization\"\n",
        "\n",
        "L = max(Lxx,Lyy,Lxy,Lyx)\n",
        "mu = min(mux,muy)\n",
        "\n",
        "\"stepsize used in SG oracle\"\n",
        "\n",
        "sgd_stepsize = mu/(4*L**2) ## step size for sgd\n",
        "print ('sgd_stepsize',sgd_stepsize)\n",
        "\n",
        "\"step size used in svrg oracle\"\n",
        "\n",
        "svrg_stepsize = mu/(21*L**2)\n",
        "print ('svrg step size',svrg_stepsize)\n",
        "\n",
        "\"Initial Kmax\"\n",
        "\n",
        "# sgd_epsilon = 0.5\n",
        "\n",
        "initial_Kmax = sgd_iteration(mux,muy,Lxx,Lyy,Lxy,Lyx,delta,nodes,lambda_max,lambda_second_small,sgd_stepsize,sgd_epsilon)\n",
        "\n",
        "# initial_Kmax = 10\n",
        "print ('sgd_iterations =',initial_Kmax)\n",
        "\n",
        "\n",
        "Tsvrg = 2000 ## number of iterations of IPDHG + SVRG\n",
        "\n",
        "threshold = 1e-08\n",
        "target_acc = 1e-08\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMW-kTfOtIG9"
      },
      "outputs": [],
      "source": [
        "## Creating files to save soutput\n",
        "\n",
        "sum_consensus_error_x = open(r\"sgd_switch_tosvrg_consensus_error_x_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "sum_consensus_error_y = open(r\"sgd_switch_tosvrg_consensus_error_y_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "total_distance_from_saddle = open(r\"sgd_switch_tosvrg_total_distance_from_saddle_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "function_values = open(r\"sgd_switch_tosvrg_function_values_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "full_batch_grad_counts = open(r\"sgd_switch_tosvrg_full_batch_grad_counts_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "compression_error_nux = open(r\"sgd_switch_tosvrg_compression_error_nux_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "compression_error_nuy = open(r\"sgd_switch_tosvrg_compression_error_nuy_num_bits_4_eps_\"+str(sgd_epsilon)+\".txt\",\"w\")\n",
        "\n",
        "\n",
        "## generate initial points randomly\n",
        "\n",
        "np.random.seed(1234)\n",
        "x0_i = np.random.random(dimension_x)\n",
        "x0 = np.tile(x0_i,(nodes,1))\n",
        "\n",
        "np.random.seed(100)\n",
        "y0_i = np.random.random(dimension_y)\n",
        "y0 = np.tile(y0_i,(nodes,1))\n",
        "\n",
        "print ('shape of x0',x0.shape)\n",
        "print ('shape of y0',y0.shape)\n",
        "\n",
        "H_x = np.copy(x0)\n",
        "H_y = np.copy(y0)\n",
        "\n",
        "D_x = np.zeros((nodes,dimension_x))\n",
        "D_y = np.zeros((nodes,dimension_y))\n",
        "\n",
        "Hw_x = oneConsensus(W,nodes,H_x)\n",
        "Hw_y = oneConsensus(W,nodes,H_y)\n",
        "\n",
        "initial_x0 = np.copy(x0)\n",
        "initial_y0 = np.copy(y0)\n",
        "\n",
        "## Loading saddle point solution zstar = (xstar,ystar) to plot the ||zt - zstar||\n",
        "## User has to change file name accordingly\n",
        "\n",
        "xstar = np.loadtxt('xstar_radiusx_100.txt')\n",
        "ystar = np.loadtxt('ystar_radiusx_100.txt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g1ytIod7tIG9"
      },
      "outputs": [],
      "source": [
        "xT, yT = heuristic_switch(A,b,x0,y0,D_x,D_y,H_x,H_y,Hw_x,Hw_y,sgd_stepsize,svrg_stepsize,initial_Kmax,\n",
        "                      sgd_epsilon,Tsvrg,threshold)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}